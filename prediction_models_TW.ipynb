{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "##### RF-based models\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump, load\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV,StratifiedKFold,GridSearchCV,cross_val_score,KFold\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "########## Quantile Regression Models\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "##### For SARIMAX and TBATS models\n",
    "from pmdarima import auto_arima\n",
    "## For outliers detection\n",
    "from sklearn import preprocessing, svm\n",
    "## For stationarity test and decomposition\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import holidays\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "### XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# parameter tuning\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "# start_time = time.time()\n",
    "# main()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "##### Import Dataset\n",
    "df_train = pd.read_csv('Data/TW_use_case/data_train_15min.csv')\n",
    "df_test = pd.read_csv('Data/TW_use_case/data_test_15min.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_days = df_train.day.unique()\n",
    "test_days = df_test.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'combine df_train and df_test'\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "df = df.reset_index(drop=True)\n",
    "df.columns = ['vendor_geohash', 'day', 'hour', 'per_15min', 'counts', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Util ##########\n",
    "def dataset_extraction(df):\n",
    "    'extracting dataset for each zone'\n",
    "    appended_data = []\n",
    "    df = df.sort_values(by = ['vendor_geohash','day','hour','per_15min'])\n",
    "    original_index = df.index\n",
    "    grid_list = df.vendor_geohash.unique()\n",
    "    for grid in grid_list:\n",
    "        df_zone = df[df['vendor_geohash'] == grid]\n",
    "        df_zone['AR1'] = df_zone['counts'].shift(1)\n",
    "        df_zone['AR2'] = df_zone['counts'].shift(2)\n",
    "        df_zone['AR3'] = df_zone['counts'].shift(3)\n",
    "        df_zone['AR4'] = df_zone['counts'].shift(4)\n",
    "        appended_data.append(df_zone)\n",
    "    appended_data = pd.concat(appended_data)\n",
    "    appended_data = appended_data.loc[original_index]\n",
    "    return appended_data\n",
    "\n",
    "def save_model(model,model_name,zone_name,model_type):\n",
    "    'model saving'\n",
    "    if model_type == 'RF':\n",
    "        # file_name = f'new_models/{model_name}_{zone_name}'\n",
    "        file_name = f'Models/{model_name}_{zone_name}'\n",
    "        dump(model, f'{file_name}.joblib')\n",
    "    \n",
    "    elif model_type == 'XGB':\n",
    "        file_name = f'Models/{model_name}_{zone_name}'\n",
    "        dump(model, f'{file_name}.joblib')\n",
    "        \n",
    "    elif model_type == 'QRF':\n",
    "        file_name = f'Models/{model_name}_{zone_name}'\n",
    "        dump(model, f'{file_name}.joblib')\n",
    "\n",
    "    elif model_type == 'SARIMA':\n",
    "        file_name = f'Models/{model_name}_{zone_name}'\n",
    "        joblib.dump(model, f'{file_name}.pkl')\n",
    "    \n",
    "    elif model_type == 'SARIMAX':\n",
    "        file_name = f'Models/{model_name}_{zone_name}'\n",
    "        joblib.dump(model, f'{file_name}.pkl')\n",
    "\n",
    "    elif model_type == 'TBATS':\n",
    "        file_name = f'Models/{model_name}_{zone_name}.sav'\n",
    "        pickle.dump(model, open(file_name, 'wb'))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def generate_predict(model, test_X, model_type = 'RF'):\n",
    "    'generate one-step ahead prediction'\n",
    "    if model_type == 'RF':\n",
    "        yhat = model.predict(test_X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def mean_absolute_scaled_error(y_true, y_pred, y_naive):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Scaled Error (MASE) for a forecasting model.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Array of actual observed values.\n",
    "    - y_pred: Array of forecasted values from the model being evaluated.\n",
    "    - y_naive: Array of forecasted values from a naive (benchmark) model.\n",
    "\n",
    "    Returns:\n",
    "    - MASE: Mean Absolute Scaled Error value.\n",
    "    \"\"\"\n",
    "    # Calculate Mean Absolute Error (MAE) for the model\n",
    "    mae_model = np.mean(np.abs(y_true - y_pred))\n",
    "    # Calculate Mean Absolute Error (MAE) for the naive (benchmark) model\n",
    "    mae_naive = np.mean(np.abs(y_true - y_naive))\n",
    "    # Calculate MASE\n",
    "    mase = mae_model / mae_naive \n",
    "    return mase\n",
    "\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred).round(3)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False).round(3)\n",
    "    r2 = r2_score(y_true, y_pred).round(3)\n",
    "    return mae, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216000\n",
      "1996283.0\n"
     ]
    }
   ],
   "source": [
    "'create complete dataset'\n",
    "df = dataset_extraction(df)\n",
    "print(len(df))\n",
    "print(sum(df.counts))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Splitting dataset into train and test'\n",
    "# drop nan values\n",
    "df = df.dropna()\n",
    "df_train = df[df.day.isin(train_days)]\n",
    "df_test = df[df.day.isin(test_days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'check NAN values'\n",
    "# print(df_train.isnull().sum())\n",
    "# print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Model modules'\n",
    "def RandomForest_predictor_with_hyperparam_tuning(df_train, df_test, model_name='LDRF'):\n",
    "    # define hyperparameters\n",
    "    param_grid_ = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 200, num = 6)],\n",
    "    'max_depth': [None, 3,4,5,6],\n",
    "    'min_samples_split': [4, 6, 8, 10],\n",
    "    'min_samples_leaf': [2, 3,5,7,10],\n",
    "    'max_features' : ['auto', 'sqrt'],\n",
    "    'bootstrap' : [True, False]}\n",
    "\n",
    "    # fit a model per grid\n",
    "    for grid in tqdm(df_train.vendor_geohash.unique()):\n",
    "        print('Training model for', grid)\n",
    "        df_train_zone = df_train[df_train.vendor_geohash == grid]\n",
    "        df_test_zone = df_test[df_test.vendor_geohash == grid]\n",
    "        if model_name == 'LDRF':\n",
    "            X_train, y_train = df_train_zone[['day_of_week','hour','AR1','AR2','AR3','AR4']], df_train_zone['counts']\n",
    "            X_test, y_test = df_test_zone[['day_of_week','hour','AR1','AR2','AR3','AR4']], df_test_zone['counts']\n",
    "        else:\n",
    "            X_train, y_train = df_train_zone[['day_of_week','hour']], df_train_zone['counts']\n",
    "            X_test, y_test = df_test_zone[['day_of_week','hour']], df_test_zone['counts']\n",
    "        \n",
    "        # define random forest regressor\n",
    "        rf = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "        # Create time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=10)\n",
    "        # create RandomizedSearchCV\n",
    "        gscv = RandomizedSearchCV(estimator=rf, \n",
    "                                    param_distributions=param_grid_, \n",
    "                                    cv=tscv, n_jobs=6, verbose=0,\n",
    "                                    n_iter=200, random_state=39,\n",
    "                                    scoring='neg_root_mean_squared_error')\n",
    "\n",
    "        start_time = time.time()\n",
    "        gscv.fit(X_train, y_train)\n",
    "        train_time_ = time.time() - start_time\n",
    "        print('Total parameter tuning time:', train_time_)\n",
    "        # Get the best hyperparameters\n",
    "        best_params = gscv.best_params_\n",
    "        print(f\"{grid} -- Best Hyperparameters:\", best_params)\n",
    "        start_train_time = time.time()\n",
    "        best_model = RandomForestRegressor(n_estimators= best_params['n_estimators'], \n",
    "                                            min_samples_split= best_params['min_samples_split'], \n",
    "                                            min_samples_leaf = best_params['min_samples_leaf'], \n",
    "                                            max_features= best_params['max_features'], \n",
    "                                            max_depth= best_params['max_depth'], \n",
    "                                            bootstrap= best_params['bootstrap'], random_state=42)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        end_train_time = time.time() - start_train_time\n",
    "        print(f\"Best Model Training Time : {end_train_time}\")\n",
    "        \n",
    "        print(f'\\n ** Training Set Results ** \\n')\n",
    "        y_pred = best_model.predict(X_train)\n",
    "        mae, rmse, r2 = evaluation(y_train, y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print(f'\\n ** Testing Set Results ** \\n')\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        mae, rmse, r2 = evaluation(y_test, y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print('Model saved for', grid)\n",
    "        save_model(best_model, model_name, grid, model_type='RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_predictor_with_hyperparam_tuning(df_train, df_test, model_name='LDXGB'):\n",
    "    'an xgboost model with hyperparameter tuning'\n",
    "    # define hyperparameters\n",
    "    param_grid_ = {'objective' : [\"reg:squarederror\"], \n",
    "                'n_estimators' : [int(x) for x in np.linspace(start = 50, stop = 200, num = 10)],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "                'max_depth': [int(x) for x in np.linspace(3, 6, num = 3)],\n",
    "                'subsample':[0.5,0.75,1]\n",
    "                }\n",
    "    # fit a model per grid\n",
    "    for grid in tqdm(df_train.vendor_geohash.unique()):\n",
    "        print('Training model for', grid)\n",
    "        xgb_regressor = xgb.XGBRegressor()\n",
    "        df_train_zone = df_train[df_train.vendor_geohash == grid]\n",
    "        df_test_zone = df_test[df_test.vendor_geohash == grid]\n",
    "        if model_name == 'LDXGB':\n",
    "            X_train, y_train = df_train_zone[['day_of_week','hour','AR1','AR2','AR3','AR4']], df_train_zone['counts']\n",
    "            X_test, y_test = df_test_zone[['day_of_week','hour','AR1','AR2','AR3','AR4']], df_test_zone['counts']\n",
    "        else:\n",
    "            X_train, y_train = df_train_zone[['day_of_week','hour']], df_train_zone['counts']\n",
    "            X_test, y_test = df_test_zone[['day_of_week','hour']], df_test_zone['counts']\n",
    "        \n",
    "        # Create time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=10)\n",
    "        # kf = KFold(n_splits=10, shuffle=False)\n",
    "            # Create GridSearchCV with the Random Forest Regressor and hyperparameter grid\n",
    "        gscv = RandomizedSearchCV(estimator=xgb_regressor, param_distributions=param_grid_, \n",
    "                                scoring='neg_root_mean_squared_error', cv=tscv, \n",
    "                                n_iter = 200, verbose=2, n_jobs = 6, random_state=42)\n",
    "        start_time = time.time()\n",
    "        gscv.fit(X_train, y_train)\n",
    "        train_time_ = time.time() - start_time\n",
    "        print('Total parameter tuning time:', train_time_)\n",
    "        # Get the best hyperparameters\n",
    "        best_params = gscv.best_params_\n",
    "        print(f\"{grid} -- Best Hyperparameters:\", best_params)\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        best_model = xgb.XGBRegressor(objective = \"reg:squarederror\", \n",
    "                                    n_estimators = best_params['n_estimators'], \n",
    "                                    learning_rate = best_params['learning_rate'], \n",
    "                                    max_depth = best_params['max_depth'], \n",
    "                                    subsample = best_params['subsample'],\n",
    "                                    nthread = 4, random_state=42)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        end_train_time = time.time() - start_train_time\n",
    "        print(f\"Best Model Training Time : {end_train_time}\")\n",
    "\n",
    "        print(f'\\n ** Training Set Results ** \\n')\n",
    "        y_pred = best_model.predict(X_train)\n",
    "        mae, rmse, r2 = evaluation(y_train, y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print(f'\\n ** Testing Set Results ** \\n')\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        mae, rmse, r2 = evaluation(y_test, y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print('Model saved for', grid)\n",
    "        save_model(best_model, model_name, grid, model_type='XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARQRF_predictor_with_hyperparam_tuning(df_train, df_test, model_name='ARQRF'):\n",
    "    # define hyperparameters\n",
    "    param_grid_ = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 200, num = 6)],\n",
    "    'max_depth': [None, 3,4,5,6],\n",
    "    'min_samples_split': [4, 6, 8, 10],\n",
    "    'min_samples_leaf': [2,3,5,7,10],\n",
    "    'max_features' : ['auto', 'sqrt'],\n",
    "    'bootstrap' : [True, False]}\n",
    "    for grid in tqdm(df_train.vendor_geohash.unique()):\n",
    "        print('Training model for', grid)\n",
    "        arqrf = RandomForestQuantileRegressor()\n",
    "        df_train_zone = df_train[df_train.vendor_geohash == grid]\n",
    "        df_test_zone = df_test[df_test.vendor_geohash == grid]\n",
    "        if model_name == 'ARQRF':\n",
    "            X_train, y_train = df_train_zone[['day_of_week','hour','AR1','AR2','AR3','AR4']], df_train_zone['counts']\n",
    "            X_test, y_test = df_test_zone[['day_of_week','hour','AR1','AR2','AR3','AR4']], df_test_zone['counts']\n",
    "        else:\n",
    "            X_train, y_train = df_train_zone[['day_of_week','hour']], df_train_zone['counts']\n",
    "            X_test, y_test = df_test_zone[['day_of_week','hour']], df_test_zone['counts']\n",
    "    \n",
    "        # Create time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=10)\n",
    "        # Random search of parameters, using 10 fold cross validation,\n",
    "        gscv = RandomizedSearchCV(estimator = arqrf,\n",
    "                                            param_distributions = param_grid_,\n",
    "                                            n_iter = 200, cv = tscv,\n",
    "                                            verbose=2, n_jobs = 6)\n",
    "        start_time = time.time()\n",
    "        gscv.fit(X_train, y_train)\n",
    "        train_time_ = time.time() - start_time\n",
    "        print('Total parameter tuning time:', train_time_)\n",
    "        # Get the best hyperparameters\n",
    "        best_params = gscv.best_params_\n",
    "        print(f\"{grid} -- Best Hyperparameters:\", best_params)\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        best_model = RandomForestQuantileRegressor(n_estimators= best_params['n_estimators'], \n",
    "                                            min_samples_split= best_params['min_samples_split'], \n",
    "                                            min_samples_leaf = best_params['min_samples_leaf'], \n",
    "                                            max_features= best_params['max_features'], \n",
    "                                            max_depth= best_params['max_depth'], \n",
    "                                            bootstrap= best_params['bootstrap'])\n",
    "        best_model.fit(X_train, y_train)\n",
    "        end_train_time = time.time() - start_train_time\n",
    "        print(f\"Best Model Training Time : {end_train_time}\")\n",
    "\n",
    "        print(f'\\n ** Training Set Results ** \\n')\n",
    "        y_pred = best_model.predict(X_train)\n",
    "        mae, rmse, r2 = evaluation(y_train, y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print(f'\\n ** Testing Set Results ** \\n')\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        mae, rmse, r2 = evaluation(y_test, y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print('Model saved for', grid)\n",
    "        save_model(best_model, model_name, grid, model_type='QRF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbats import TBATS, BATS\n",
    "def TBATS_training(df_train, df_test, model_name='TBATS'):\n",
    "    'takes a long time to train'\n",
    "    for grid in tqdm(df_train.vendor_geohash.unique()):\n",
    "        print('Training model for', grid)\n",
    "        df_train_zone = df_train[df_train.vendor_geohash == grid]\n",
    "        df_test_zone = df_test[df_test.vendor_geohash == grid]\n",
    "        start_train_time = time.time()\n",
    "        estimator = TBATS(seasonal_periods=(96, 672)) # 96 for daily seasonality and 672 for weekly seasonality\n",
    "        model = estimator.fit(df_train_zone['counts'][-2688:])\n",
    "        end_train_time = time.time() - start_train_time\n",
    "        print(f\"Best Model Training Time : {end_train_time}\")\n",
    "\n",
    "        print(f'\\n ** Testing Set Results ** \\n')\n",
    "        y_pred = model.forecast(steps=len(df_test_zone))\n",
    "        mae, rmse, r2 = evaluation(df_test_zone['counts'], y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print('Model saved for', grid)\n",
    "        save_model(model, model_name, grid, model_type='TBATS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARIMA_training(df_train, df_test, model_name='SARIMA'):\n",
    "    'takes a long time to train'\n",
    "    for grid in tqdm(df_train.vendor_geohash.unique()):\n",
    "        print('Training model for', grid)\n",
    "        df_train_zone = df_train[df_train.vendor_geohash == grid][-2688:]\n",
    "        df_test_zone = df_test[df_test.vendor_geohash == grid]\n",
    "        start_train_time = time.time()\n",
    "        model = auto_arima(df_train_zone['counts'], seasonal=True, \n",
    "                           m=96, stepwise=False, \n",
    "                           suppress_warnings=True, n_jobs=4)\n",
    "        end_train_time = time.time() - start_train_time\n",
    "        print(f\"Best Model Training Time : {end_train_time}\")\n",
    "\n",
    "        print(f'\\n ** Testing Set Results ** \\n')\n",
    "        y_pred = model.predict(n_periods=len(df_test_zone))\n",
    "        mae, rmse, r2 = evaluation(df_test_zone['counts'], y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print('Model saved for', grid)\n",
    "        save_model(model, model_name, grid, model_type='SARIMA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARIMAX_training(df_train, df_test, model_name='SARIMA'):\n",
    "    for grid in tqdm(df_train.vendor_geohash.unique()):\n",
    "        print('Training model for', grid)\n",
    "        df_train_zone = df_train[df_train.vendor_geohash == grid][-2688:]\n",
    "        df_test_zone = df_test[df_test.vendor_geohash == grid]\n",
    "        start_train_time = time.time()\n",
    "        model = auto_arima(df_train_zone['counts'], exogenous=df_train_zone[['day_of_week','hour']], seasonal=True, m=96, stepwise=True, trace=True)\n",
    "        end_train_time = time.time() - start_train_time\n",
    "        print(f\"Best Model Training Time : {end_train_time}\")\n",
    "\n",
    "        print(f'\\n ** Testing Set Results ** \\n')\n",
    "        y_pred = model.predict(n_periods=len(df_test_zone), exogenous=df_test_zone[['day_of_week','hour']])\n",
    "        mae, rmse, r2 = evaluation(df_test_zone['counts'], y_pred)\n",
    "        print('MAE:', mae, 'RMSE:', rmse, 'R2:', r2)\n",
    "\n",
    "        print('Model saved for', grid)\n",
    "        save_model(model, model_name, grid, model_type='SARIMAX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBATS_training(df_train, df_test, model_name='TBATS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARIMA_training(df_train, df_test, model_name='SARIMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARIMAX_training(df_train, df_test, model_name='SARIMAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQRF_predictor_with_hyperparam_tuning(df_train, df_test, model_name='ARQRF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_predictor_with_hyperparam_tuning(df_train, df_test, model_name='LDXGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_predictor_with_hyperparam_tuning(df_train, df_test, model_name='LDRF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
