{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import networkx as nx\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "# import holidays\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred, squared=False):\n",
    "    if squared:\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "    else:\n",
    "        return np.sqrt(np.mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(actual,predicted):\n",
    "    squared_diffs = np.zeros(len(predicted))\n",
    "    for i in range(len(predicted)):\n",
    "        squared_diffs = (np.log(predicted[i]+1) - np.log(actual[i]+1))**2\n",
    "    rmsle = np.sqrt(np.mean(squared_diffs))\n",
    "    return rmsle\n",
    "\n",
    "def variance_residual(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    return round(np.std(residuals), 3)\n",
    "\n",
    "def prediction_evaluate(y_pred, y_true):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    rmsle = root_mean_squared_log_error(y_true, y_pred)\n",
    "    resid_std = variance_residual(y_true, y_pred)\n",
    "    return mae, rmse, rmsle, resid_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "    # return mean_absolute_error(y_true, y_pred).round(3)\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    # return mean_squared_error(y_true, y_pred, squared=False).round(3)\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.log1p(y_true+1) - np.log1p(y_pred+1))**2))\n",
    "\n",
    "def MAPE(y_true, y_pred, c=1):\n",
    "    return np.mean(np.abs((y_true - y_pred +c) / (y_true+c)) * 100)\n",
    "\n",
    "def AE(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred)\n",
    "\n",
    "def prediction_evaluate(y_pred, y_true):\n",
    "    mae = MAE(y_true, y_pred)\n",
    "    rmse = RMSE(y_true, y_pred)\n",
    "    rmsle = RMSLE(y_true, y_pred)\n",
    "    # mape = MAPE(y_true, y_pred)\n",
    "    # ae = AE(y_true, y_pred)\n",
    "    resid_std = variance_residual(y_true, y_pred)\n",
    "    return mae, rmse, rmsle, resid_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph of h3 grids and their connectivity info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 7),\n",
       " (1, 11),\n",
       " (2, 3),\n",
       " (2, 10),\n",
       " (2, 15),\n",
       " (3, 8),\n",
       " (3, 15),\n",
       " (10, 12),\n",
       " (10, 18),\n",
       " (8, 15),\n",
       " (4, 14),\n",
       " (4, 16),\n",
       " (14, 16),\n",
       " (14, 17),\n",
       " (16, 17),\n",
       " (5, 9),\n",
       " (5, 11),\n",
       " (5, 16),\n",
       " (5, 17),\n",
       " (5, 19),\n",
       " (9, 11),\n",
       " (9, 19),\n",
       " (17, 19),\n",
       " (6, 13),\n",
       " (13, 14),\n",
       " (13, 17),\n",
       " (12, 18),\n",
       " (18, 19)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load zone address from txt file\n",
    "zone_addresses = None\n",
    "\n",
    "# find the h3 index for each zone\n",
    "h3_indexes = [h3.latlng_to_cell(lat, lon, 8) for (lat, lon) in zone_addresses.values()]\n",
    "\n",
    "# a function to check whether two zones are neighbors\n",
    "def are_zones_neighbors(h1, h2):\n",
    "    return h3.are_neighbor_cells(h1, h2)  \n",
    "\n",
    "def return_connected_zones_index_on_list(h3_indexes):\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(h3_indexes)):\n",
    "        for j in range(i + 1, len(h3_indexes)):\n",
    "            if are_zones_neighbors(h3_indexes[i], h3_indexes[j]):\n",
    "                G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "zone_graph = return_connected_zones_index_on_list(h3_indexes)\n",
    "\n",
    "# check the connected nodes for each node on zone_graph\n",
    "connected_pairs = []\n",
    "for node in zone_graph.nodes():\n",
    "    neighbors = list(zone_graph.neighbors(node))\n",
    "    # print(f\"Node {node} is connected to {neighbors}\")\n",
    "    # for each pair of neighbor, record it as a tuple\n",
    "    for i in range(len(neighbors)):\n",
    "        connected_pairs.append((node, neighbors[i]))\n",
    "    # if (i,j) is inlcuded in connected_pairs, keep only (i,j), remove (j,i)\n",
    "    connected_pairs = [(i,j) for (i,j) in connected_pairs if i < j]\n",
    "\n",
    "connected_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6468.0\n",
      "308.0\n"
     ]
    }
   ],
   "source": [
    "df_15min = pd.read_csv('Data/case 1/processed_data.csv')\n",
    "df_test = df_15min.iloc[-20*308:]\n",
    "df_train_21w = df_15min.iloc[-20*308*22:-20*308]\n",
    "# df_train_4w = df_15min.iloc[-20*308*5:-20*308]\n",
    "print(len(df_train_21w)/20)\n",
    "# print(len(df_train_4w)/20)\n",
    "print(len(df_test)/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>OrZone</th>\n",
       "      <th>temp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>prep</th>\n",
       "      <th>Is_Holiday</th>\n",
       "      <th>AR1</th>\n",
       "      <th>AR2</th>\n",
       "      <th>AR3</th>\n",
       "      <th>AR4</th>\n",
       "      <th>counts</th>\n",
       "      <th>zone_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138520</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>zone 1</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138521</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>zone 10</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138522</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>zone 11</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138523</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>zone 12</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138524</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>zone 13</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  DayofWeek  Hour  Quarter   OrZone  temp  wspd  prep  \\\n",
       "138520  2020-09-07          0    21        1   zone 1   154    40     0   \n",
       "138521  2020-09-07          0    21        1  zone 10   154    40     0   \n",
       "138522  2020-09-07          0    21        1  zone 11   154    40     0   \n",
       "138523  2020-09-07          0    21        1  zone 12   154    40     0   \n",
       "138524  2020-09-07          0    21        1  zone 13   154    40     0   \n",
       "\n",
       "        Is_Holiday  AR1  AR2  AR3  AR4  counts  zone_id  \n",
       "138520           0  0.0  1.0  0.0  2.0     1.0        2  \n",
       "138521           0  0.0  0.0  0.0  1.0     0.0        7  \n",
       "138522           0  0.0  0.0  1.0  2.0     0.0       15  \n",
       "138523           0  2.0  3.0  3.0  2.0     0.0        3  \n",
       "138524           0  1.0  3.0  3.0  2.0     0.0       19  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_15min = pd.read_csv('Data/case 1/processed_data.csv')\n",
    "df_15min_train = df_15min.iloc[-20*308*22:-20*308]\n",
    "df_15min = df_15min.iloc[-20*308:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create historical avergae dictionary\n",
    "list_of_ha_dict_21 = []\n",
    "for z in range(20):\n",
    "    df_train_21w_ = df_train_21w[df_train_21w.OrZone == f'zone {z+1}']\n",
    "    df_train_21w_['wdhr'] = [(x, y) for x, y in zip(df_train_21w_['DayofWeek'], df_train_21w_['Hour'])]\n",
    "    temp = df_train_21w_.groupby('wdhr')['counts'].mean()\n",
    "    list_of_ha_dict_21.append(temp.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRPS evaluation for HA and LDQRF, QRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model(num_week, model_name,zone_name):\n",
    "    'model reading'\n",
    "    file_name = f'Case_1_Models/week_{num_week}/{model_name}_{zone_name}.joblib'\n",
    "    model = load(file_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'probabilistic prediction evaluation: CRPD'\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def CRPS(df_, pred_model, quantiles, with_LD = True):\n",
    "    'continuous ranked probability score'\n",
    "    def empirical_cdf(predicted_values, quantiles):\n",
    "        # Construct empirical CDF from predicted quantiles\n",
    "        def cdf(x):\n",
    "            return np.interp(x, predicted_values, quantiles, left=0, right=1)\n",
    "        return cdf\n",
    "    # Define CRPS calculation\n",
    "    def crps(cdf_function, observed_value):\n",
    "        def integrand(z):\n",
    "            return (cdf_function(z) - (z >= observed_value)) ** 2\n",
    "        crps_value, _ = quad(integrand, -np.inf, np.inf)\n",
    "        return crps_value\n",
    "    y_true = df_['counts'].values\n",
    "    if with_LD:\n",
    "        X_ = df_[['DayofWeek','Hour','temp', 'wspd', 'prep', 'Is_Holiday','AR1','AR2','AR3','AR4']]\n",
    "    else:\n",
    "        X_ = df_[['DayofWeek','Hour','temp', 'wspd', 'prep', 'Is_Holiday']]\n",
    "    crps_values = np.zeros(len(y_true))\n",
    "    for i in tqdm(range(len(y_true))):\n",
    "        predicted_values = pred_model.predict(X_.iloc[i].values.reshape(1,-1), quantiles=quantiles)[0]\n",
    "        observed_value = y_true[i]\n",
    "        cdf_function = empirical_cdf(predicted_values, quantiles)\n",
    "        crps_values[i] = crps(cdf_function, observed_value)\n",
    "    return crps_values\n",
    "\n",
    "def CRPS_evaluation_per_grid_LDQRF(df_train, df_test, order_of_grids, \n",
    "                                   num_week = 4, model_name = 'LDQRF',\n",
    "                                   quantiles=[0.1, 0.3, 0.5, 0.7, 0.9]):\n",
    "    length = 20\n",
    "    crps_train = np.zeros(length)\n",
    "    crps_train_std = np.zeros(length)\n",
    "    crps_test = np.zeros(length)\n",
    "    crps_test_std = np.zeros(length)\n",
    "\n",
    "    for i, grid in tqdm(enumerate(order_of_grids)):\n",
    "        df_train_grid = df_train[df_train['OrZone'] == grid]\n",
    "        df_test_grid = df_test[df_test['OrZone'] == grid]\n",
    "        model = read_model(num_week = num_week, model_name=model_name,zone_name=grid)\n",
    "        if 'LD' in model_name:\n",
    "            crps_train_temp = CRPS(df_train_grid, model, quantiles, with_LD = True)\n",
    "            crps_test_temp = CRPS(df_test_grid, model, quantiles, with_LD = True)\n",
    "        else:\n",
    "            crps_train_temp = CRPS(df_train_grid, model, quantiles, with_LD = False)\n",
    "            crps_test_temp = CRPS(df_test_grid, model, quantiles, with_LD = False)\n",
    "\n",
    "        crps_train[i] = np.mean(crps_train_temp)\n",
    "        crps_train_std[i] = np.std(crps_train_temp)\n",
    "\n",
    "        crps_test[i] = np.mean(crps_test_temp)\n",
    "        crps_test_std[i] = np.std(crps_test_temp)\n",
    "        \n",
    "    print(f'Train: CRPS: {np.mean(crps_train):.3f}({np.std(crps_train):.3f})')\n",
    "    print(f'Test: CRPS: {np.mean(crps_test):.3f}({np.std(crps_test):.3f})')\n",
    "    \n",
    "    return crps_train, crps_test, crps_train_std, crps_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LDQRF - 21 week training'\n",
    "\n",
    "order_of_grids = [f'zone {i+1}' for i in range(20)]\n",
    "quantiles_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "crps_train, crps_test, crps_train_std, crps_test_std = CRPS_evaluation_per_grid_LDQRF(df_train_21w, df_test, order_of_grids,\n",
    "                                                                                    num_week = 21, model_name = 'LDQRF',\n",
    "                                                                                    quantiles = quantiles_list)\n",
    "\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_train_21week_LDQRF.npy', crps_train)\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_test_21week_LDQRF.npy', crps_test)\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_train_std_21week_LDQRF.npy', crps_train_std)\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_test_std_21week_LDQRF.npy', crps_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'QRF - 21 week training'\n",
    "\n",
    "order_of_grids = [f'zone {i+1}' for i in range(20)]\n",
    "quantiles_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "crps_train, crps_test, crps_train_std, crps_test_std = CRPS_evaluation_per_grid_LDQRF(df_train_21w, df_test, order_of_grids,\n",
    "                                                                                    num_week = 21, model_name = 'QRF',\n",
    "                                                                                    quantiles = quantiles_list)\n",
    "\n",
    "\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_train_21week_QRF.npy', crps_train)\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_test_21week_QRF.npy', crps_test)\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_train_std_21week_QRF.npy', crps_train_std)\n",
    "np.save('Results_Saving_Case_1/CRPS/crps_test_std_21week_QRF.npy', crps_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create quantile predictions from the weekday-hour grouped data points\n",
    "def weekday_hour_quantile(df, quantiles= [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    df = df[['OrZone','DayofWeek', 'Hour','counts']]\n",
    "    grouped = df.groupby(['OrZone','DayofWeek', 'Hour'])\n",
    "    # Function to calculate quantiles\n",
    "    def calculate_quantiles(group):\n",
    "        return group['counts'].quantile(quantiles)\n",
    "    # Apply the function to each group\n",
    "    quantile_results = grouped.apply(calculate_quantiles).reset_index()\n",
    "    quantile_dict = {}\n",
    "\n",
    "    for index, row in quantile_results.iterrows():\n",
    "        key = (row['OrZone'], row['DayofWeek'], row['Hour'])\n",
    "        values = row[3:].values.astype(float)\n",
    "        quantile_dict[key] = values\n",
    "    return quantile_dict\n",
    "\n",
    "# Example function to retrieve quantiles\n",
    "def get_quantiles(quantile_dict, grid, day_of_week, hour):\n",
    "    key = (grid, day_of_week, hour)\n",
    "    return quantile_dict.get(key, np.array([]))  # Return an empty array if key not found\n",
    "\n",
    "def CRPS_historical(df_, quantile_dict, quantiles):\n",
    "    'continuous ranked probability score'\n",
    "    def empirical_cdf(predicted_values, quantiles):\n",
    "        # Construct empirical CDF from predicted quantiles\n",
    "        def cdf(x):\n",
    "            return np.interp(x, predicted_values, quantiles, left=0, right=1)\n",
    "        return cdf\n",
    "    # Define CRPS calculation\n",
    "    def crps(cdf_function, observed_value):\n",
    "        def integrand(z):\n",
    "            return (cdf_function(z) - (z >= observed_value)) ** 2\n",
    "        crps_value, _ = quad(integrand, -np.inf, np.inf)\n",
    "        return crps_value\n",
    "    \n",
    "    y_true = df_['counts'].values\n",
    "    X_ = df_[['OrZone', 'DayofWeek','Hour']]\n",
    "    crps_values = np.zeros(len(y_true))\n",
    "    for i in tqdm(range(len(y_true))):\n",
    "        predicted_values = get_quantiles(quantile_dict, X_.iloc[i].OrZone, X_.iloc[i].DayofWeek, X_.iloc[i].Hour)\n",
    "        observed_value = y_true[i]\n",
    "        cdf_function = empirical_cdf(predicted_values, quantiles)\n",
    "        temp = crps(cdf_function, observed_value)\n",
    "        crps_values[i] = temp\n",
    "    return crps_values\n",
    "\n",
    "def CRPS_evaluation_per_grid_HA(df_train, df_test, order_of_grids, quantile_dict, quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    length = 20\n",
    "    crps_train = np.zeros(length)\n",
    "    crps_train_std = np.zeros(length)\n",
    "    crps_test = np.zeros(length)\n",
    "    crps_test_std = np.zeros(length)\n",
    "\n",
    "    for i, grid in tqdm(enumerate(order_of_grids)):\n",
    "        df_train_grid = df_train[df_train['OrZone'] == grid]\n",
    "        df_test_grid = df_test[df_test['OrZone'] == grid]\n",
    "        # model = read_model(model_name='ARQRF',zone_name=grid)\n",
    "        crps_train_temp = CRPS_historical(df_train_grid, quantile_dict, quantiles)\n",
    "        crps_train[i] = np.mean(crps_train_temp)\n",
    "        crps_train_std[i] = np.std(crps_train_temp)\n",
    "        crps_test_temp = CRPS_historical(df_test_grid, quantile_dict, quantiles)\n",
    "        crps_test[i] = np.mean(crps_test_temp)\n",
    "        crps_test_std[i] = np.std(crps_test_temp)\n",
    "\n",
    "    print(f'Train: CRPS: {np.mean(crps_train):.3f}({np.std(crps_train):.3f})')\n",
    "    print(f'Test: CRPS: {np.mean(crps_test):.3f}({np.std(crps_test):.3f})')\n",
    "    \n",
    "    return crps_train, crps_test, crps_train_std, crps_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Historical: 21 week - Day-of-a-week, Hour-of-a-day Quantile Prediction'\n",
    "\n",
    "quantile_dict = weekday_hour_quantile(df_train_21w, quantiles_list)\n",
    "crps_train, crps_test, crps_train_std, crps_test_std = CRPS_evaluation_per_grid_HA(df_train_21w, df_test, order_of_grids, \n",
    "                                                                                   quantile_dict, quantiles_list)\n",
    "\n",
    "\n",
    "np.save('Results_Saving_Case_1/CRPS/CRPS_train_historical_21week.npy', crps_train)\n",
    "np.save('Results_Saving_Case_1/CRPS/CRPS_test_historical_21week.npy', crps_test)\n",
    "np.save('Results_Saving_Case_1/CRPS/CRPS_train_std_historical_21week.npy', crps_train_std)\n",
    "np.save('Results_Saving_Case_1/CRPS/CRPS_test_std_historical_21week.npy', crps_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_train_LDQRF = np.load('Results_Saving_Case_1/CRPS/CRPS_train.npy') \n",
    "crps_test_LDQRF = np.load('Results_Saving_Case_1/CRPS/CRPS_test.npy')\n",
    "crps_peak_LDQRF = np.load('Results_Saving_Case_1/CRPS/CRPS_peak.npy')\n",
    "crps_train_std_LDQRF = np.load('Results_Saving_Case_1/CRPS/CRPS_train_std.npy')\n",
    "crps_test_std_LDQRF = np.load('Results_Saving_Case_1/CRPS/CRPS_test_std.npy')\n",
    "crps_peak_std_LDQRF = np.load('Results_Saving_Case_1/CRPS/CRPS_peak_std.npy')\n",
    "\n",
    "crps_train_HA = np.load('Results_Saving_Case_1/CRPS/CRPS_train_historical.npy')\n",
    "crps_test_HA = np.load('Results_Saving_Case_1/CRPS/CRPS_test_historical.npy')\n",
    "crps_peak_HA = np.load('Results_Saving_Case_1/CRPS/CRPS_peak_historical.npy')\n",
    "crps_train_std_HA = np.load('Results_Saving_Case_1/CRPS/CRPS_train_std_historical.npy')\n",
    "crps_test_std_HA = np.load('Results_Saving_Case_1/CRPS/CRPS_test_std_historical.npy')\n",
    "crps_peak_std_HA = np.load('Results_Saving_Case_1/CRPS/CRPS_peak_std_historical.npy')\n",
    "\n",
    "evas = [crps_train_LDQRF, crps_train_HA, crps_test_LDQRF, crps_test_HA]\n",
    "stds = [crps_train_std_LDQRF, crps_train_std_HA, crps_test_std_LDQRF, crps_test_std_HA]\n",
    "test_names = [f'LDQRF: {np.mean(crps_train_LDQRF):.3f} ({np.mean(crps_train_std_LDQRF):.3f})', \n",
    "              f'Historical: {np.mean(crps_train_HA):.3f} ({np.mean(crps_train_std_HA):.3f})', \n",
    "              f'LDQRF: {np.mean(crps_test_LDQRF):.3f} ({np.mean(crps_test_std_LDQRF):.3f})',  \n",
    "              f'Historical: {np.mean(crps_test_HA):.3f} ( {np.mean(crps_test_std_HA):.3f})']\n",
    "zone_names = [f'{i+1}' for i in range(25)]\n",
    "colors = ['darkmagenta', 'orchid', 'darkgreen',  'mediumseagreen']\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(9, 6))  # 1 row, 2 columns\n",
    "\n",
    "# Plot for training set\n",
    "for i, name in enumerate(test_names[:2]):  # Only take the first two elements for training\n",
    "    axs[0].plot(zone_names, evas[i], 'X-', label=test_names[i], color=colors[i])\n",
    "\n",
    "axs[0].set_xlabel(\"Pick-up Zone\", fontsize=14)\n",
    "axs[0].set_ylabel(\"CRPS\", fontsize=14)\n",
    "axs[0].tick_params(labelsize=13)\n",
    "axs[0].legend(fontsize=12, ncol=1)\n",
    "axs[0].set_title(\"CPRS measured over the training set of case: Delivery Hero\")\n",
    "\n",
    "# Plot for testing set\n",
    "for i, name in enumerate(test_names[2:]):  # Only take the last two elements for testing\n",
    "    axs[1].plot(zone_names, evas[i+2], 'X-', label=test_names[i+2], color=colors[i+2])\n",
    "\n",
    "axs[1].set_xlabel(\"Pick-up Zone\", fontsize=14)\n",
    "axs[1].set_ylabel(\"CRPS\", fontsize=14)\n",
    "axs[1].tick_params(labelsize=13)\n",
    "axs[1].legend(fontsize=12, ncol=1)\n",
    "axs[1].set_title(\"CPRS measured over the testing set of case: Delivery Hero\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_TBATS_models = {}\n",
    "all_RF_models = {}\n",
    "all_ARRF_models = {}\n",
    "all_XGB_models = {}\n",
    "all_ARXGB_models = {}\n",
    "all_QRF_models = {}\n",
    "all_LDQRF_models = {}\n",
    "\n",
    "# read trained models - model trained by 21-week data\n",
    "for i in range(20):\n",
    "    zone_name = f'zone {i+1}'\n",
    "    all_TBATS_models[zone_name] = joblib.load(f'Case_1_Models/TBATS/{zone_name}.sav')\n",
    "    all_RF_models[zone_name] = joblib.load(f'Case_1_Models/week_21/RF_{zone_name}.joblib')\n",
    "    all_ARRF_models[zone_name]= joblib.load(f'Case_1_Models/week_21/LDRF_{zone_name}.joblib')\n",
    "    all_XGB_models[zone_name] = joblib.load(f'Case_1_Models/week_21/XGB_{zone_name}.joblib')\n",
    "    all_ARXGB_models[zone_name] = joblib.load(f'Case_1_Models/week_21/LDXGB_{zone_name}.joblib')\n",
    "    all_QRF_models[zone_name] = joblib.load(f'Case_1_Models/week_21/QRF_{zone_name}.joblib')\n",
    "    all_LDQRF_models[zone_name] = joblib.load(f'Case_1_Models/week_21/LDQRF_{zone_name}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predicted attributes for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(X_test, predictor, model=None, q_=None, HA_dict=None):\n",
    "    if predictor == 'QRF':\n",
    "        X_test = X_test.drop(['AR1', 'AR2', 'AR3', 'AR4'], axis=1)\n",
    "        y_pred = model.predict(X_test, quantiles=q_)\n",
    "    elif predictor == 'LDQRF':\n",
    "        y_pred = model.predict(X_test, quantiles=q_)\n",
    "    elif predictor == 'RF':\n",
    "        X_test = X_test.drop(['AR1', 'AR2', 'AR3', 'AR4'], axis=1)\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif predictor == 'LDRF':\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif predictor == 'RF':\n",
    "        X_test = X_test.drop(['AR1', 'AR2', 'AR3', 'AR4'], axis=1)\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif predictor == 'LDXGB':\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif predictor == 'XGB':\n",
    "        X_test = X_test.drop(['AR1', 'AR2', 'AR3', 'AR4'], axis=1)\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif predictor == 'TBATS':\n",
    "        y_pred = model.forecast(steps=len(X_test))\n",
    "    elif predictor == 'HA':\n",
    "        X_test['wdhr'] = [(x, y) for x, y in zip(X_test['DayofWeek'], X_test['Hour'])]\n",
    "        X_test['HA'] = X_test.wdhr.map(HA_dict)\n",
    "        y_pred = X_test['HA'].values\n",
    "    elif predictor == 'Myopic':\n",
    "        y_pred = X_test['AR1'].values\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'evaluate deterministic prediction performance of models'\n",
    "\n",
    "# note that a good benchmark will be historical average according to the zone's weekday and hour\n",
    "def forecasting_per_grid(df_test, dict_of_predictors={}, predictor='myopic',quantile_ = 0.5, list_of_ha_dict=None, print_ = True):\n",
    "    length = 20\n",
    "    mae_test, rmse_test, rmsle_test, resid_std_test = np.zeros(length),  np.zeros(length), np.zeros(length), np.zeros(length)\n",
    "    QRF_names = ['QRF','LDQRF']\n",
    "    list_of_data = []\n",
    "    for i in range(20):\n",
    "        grid = f'zone {i+1}'\n",
    "        df_test_grid = df_test[df_test['OrZone'] == grid]\n",
    "        X_test = df_test_grid[['DayofWeek', 'Hour', 'temp', 'wspd', 'prep', 'Is_Holiday', 'AR1', 'AR2', 'AR3', 'AR4']]\n",
    "        y_test = df_test_grid['counts'].values\n",
    "        if predictor in QRF_names:\n",
    "            model = dict_of_predictors[grid]\n",
    "            pred_test = get_pred(X_test, predictor, model=model, q_=quantile_, HA_dict=None)\n",
    "            df_test_grid[f'{predictor}_{quantile_}'] = pred_test\n",
    "        elif predictor == 'HA':\n",
    "            pred_test = get_pred(X_test, predictor, model=None, q_=None, HA_dict=list_of_ha_dict[i])\n",
    "            df_test_grid[predictor] = pred_test\n",
    "        elif predictor == 'Myopic':\n",
    "            pred_test = get_pred(X_test, predictor, model=None, q_=None, HA_dict=None)\n",
    "            df_test_grid[predictor] = pred_test\n",
    "        else:\n",
    "            model = dict_of_predictors[grid]\n",
    "            pred_test = get_pred(X_test, predictor, model=model, q_=None, HA_dict=None)\n",
    "            df_test_grid[predictor] = pred_test\n",
    "        if print_:\n",
    "            mae_test[i], rmse_test[i], rmsle_test[i], resid_std_test[i] = prediction_evaluate(pred_test, y_test)\n",
    "        list_of_data.append(df_test_grid)\n",
    "\n",
    "    if print_:\n",
    "        print(f'Test: MAE: {np.mean(mae_test):.3f}({np.std(mae_test):.3f}), \\n RMSE: {np.mean(rmse_test):.3f}({np.std(rmse_test):.3f}), \\n RMSLE: {np.mean(rmsle_test):.3f}({np.std(rmsle_test):.3f}), \\n Residual Std: {np.mean(resid_std_test):.3f}({np.std(resid_std_test):.3f})')\n",
    "        print('\\n')\n",
    "    \n",
    "    # create new dataframe from list_of_data\n",
    "    df_test = pd.concat(list_of_data)\n",
    "    # re-arrange rows based on date, hour, quarter, then OrZone\n",
    "    df_test = df_test.sort_values(by=['date', 'Hour', 'Quarter', 'OrZone'])\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (21W): MAE: 0.851(0.290), \n",
      " RMSE: 1.209(0.382), \n",
      " RMSLE: 0.431(0.262), \n",
      " Residual Std: 1.209(0.382)\n",
      "\n",
      "\n",
      "Testing (21W): MAE: 0.837(0.269), \n",
      " RMSE: 1.159(0.337), \n",
      " RMSLE: 0.395(0.229), \n",
      " Residual Std: 1.148(0.339)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 20\n",
    "mae_test, rmse_test, rmsle_test, resid_std_test = np.zeros(length),  np.zeros(length), np.zeros(length), np.zeros(length)\n",
    "for i in range(length):\n",
    "    grid = f'zone {i+1}'\n",
    "    df_test_grid = df_train_21w[df_train_21w['OrZone'] == grid]\n",
    "    X_ = df_test_grid[['DayofWeek', 'Hour', 'temp', 'wspd', 'prep', 'Is_Holiday', 'AR1', 'AR2', 'AR3', 'AR4']]\n",
    "    y_ = df_test_grid['counts'].values\n",
    "    pred_test = get_pred(X_, 'HA', model=None, q_=None, HA_dict=list_of_ha_dict_21[i])\n",
    "    mae_test[i], rmse_test[i], rmsle_test[i], resid_std_test[i] = prediction_evaluate(pred_test, y_)\n",
    "print(f'Training (21W): MAE: {np.mean(mae_test):.3f}({np.std(mae_test):.3f}), \\n RMSE: {np.mean(rmse_test):.3f}({np.std(rmse_test):.3f}), \\n RMSLE: {np.mean(rmsle_test):.3f}({np.std(rmsle_test):.3f}), \\n Residual Std: {np.mean(resid_std_test):.3f}({np.std(resid_std_test):.3f})')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "length = 20\n",
    "mae_test, rmse_test, rmsle_test, resid_std_test = np.zeros(length),  np.zeros(length), np.zeros(length), np.zeros(length)\n",
    "for i in range(length):\n",
    "    grid = f'zone {i+1}'\n",
    "    df_test_grid = df_test[df_test['OrZone'] == grid]\n",
    "    X_ = df_test_grid[['DayofWeek', 'Hour', 'temp', 'wspd', 'prep', 'Is_Holiday', 'AR1', 'AR2', 'AR3', 'AR4']]\n",
    "    y_ = df_test_grid['counts'].values\n",
    "    pred_test = get_pred(X_, 'HA', model=None, q_=None, HA_dict=list_of_ha_dict_21[i])\n",
    "    mae_test[i], rmse_test[i], rmsle_test[i], resid_std_test[i] = prediction_evaluate(pred_test, y_)\n",
    "print(f'Testing (21W): MAE: {np.mean(mae_test):.3f}({np.std(mae_test):.3f}), \\n RMSE: {np.mean(rmse_test):.3f}({np.std(rmse_test):.3f}), \\n RMSLE: {np.mean(rmsle_test):.3f}({np.std(rmsle_test):.3f}), \\n Residual Std: {np.mean(resid_std_test):.3f}({np.std(resid_std_test):.3f})')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing (4W): MAE: 1.290(0.412), \n",
      " RMSE: 1.634(0.520), \n",
      " RMSLE: 0.556(0.346), \n",
      " Residual Std: 1.607(0.517)\n",
      "\n",
      "\n",
      "Testing (21W): MAE: 1.290(0.412), \n",
      " RMSE: 1.634(0.520), \n",
      " RMSLE: 0.556(0.346), \n",
      " Residual Std: 1.607(0.517)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'weekend dinner time'\n",
    "dinner_hours = [17, 18, 19, 20]\n",
    "df_test_dinner = df_test[(df_test['DayofWeek'] >= 5) & (df_test['Hour'].isin(dinner_hours))]\n",
    "dict_of_predictors = all_ARRF_models\n",
    "\n",
    "length = 20\n",
    "mae_test, rmse_test, rmsle_test, resid_std_test = np.zeros(length),  np.zeros(length), np.zeros(length), np.zeros(length)\n",
    "for i in range(length):\n",
    "    grid = f'zone {i+1}'\n",
    "    df_test_grid = df_test_dinner[df_test_dinner['OrZone'] == grid]\n",
    "    X_ = df_test_grid[['DayofWeek', 'Hour', 'temp', 'wspd', 'prep', 'Is_Holiday', 'AR1', 'AR2', 'AR3', 'AR4']]\n",
    "    y_ = df_test_grid['counts'].values\n",
    "    model = dict_of_predictors[grid]\n",
    "    pred_test = get_pred(X_, 'LDRF', model=model, q_=None, HA_dict=list_of_ha_dict_21[i])\n",
    "    mae_test[i], rmse_test[i], rmsle_test[i], resid_std_test[i] = prediction_evaluate(pred_test, y_)\n",
    "print(f'Testing (21W): MAE: {np.mean(mae_test):.3f}({np.std(mae_test):.3f}), \\n RMSE: {np.mean(rmse_test):.3f}({np.std(rmse_test):.3f}), \\n RMSLE: {np.mean(rmsle_test):.3f}({np.std(rmsle_test):.3f}), \\n Residual Std: {np.mean(resid_std_test):.3f}({np.std(resid_std_test):.3f})')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 20\n",
    "mae_test, rmse_test, rmsle_test, resid_std_test = np.zeros(length),  np.zeros(length), np.zeros(length), np.zeros(length)\n",
    "for i in range(length):\n",
    "    grid = f'zone {i+1}'\n",
    "    df_test_grid = df_train_21w[df_train_21w['OrZone'] == grid]\n",
    "    X_ = df_test_grid[['DayofWeek', 'Hour', 'temp', 'wspd', 'prep', 'Is_Holiday', 'AR1', 'AR2', 'AR3', 'AR4']]\n",
    "    y_ = df_test_grid['counts'].values\n",
    "    pred_test = get_pred(X_, 'Myopic', model=None, q_=None, HA_dict=None)\n",
    "    mae_test[i], rmse_test[i], rmsle_test[i], resid_std_test[i] = prediction_evaluate(pred_test, y_)\n",
    "print(f'Training (21W): MAE: {np.mean(mae_test):.3f}({np.std(mae_test):.3f}), \\n RMSE: {np.mean(rmse_test):.3f}({np.std(rmse_test):.3f}), \\n RMSLE: {np.mean(rmsle_test):.3f}({np.std(rmsle_test):.3f}), \\n Residual Std: {np.mean(resid_std_test):.3f}({np.std(resid_std_test):.3f})')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min, dict_of_predictors=None, \n",
    "                                predictor='HA',quantile_ = None, list_of_ha_dict=list_of_ha_dict_21, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=None, \n",
    "                                predictor='Myopic',quantile_ = None, list_of_ha_dict=None, print_ = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_TBATS_models, \n",
    "                                predictor='TBATS',quantile_ = None, list_of_ha_dict=None, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_ARXGB_models, \n",
    "                                predictor='LDXGB',quantile_ = None, list_of_ha_dict=None, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_ARRF_models, \n",
    "                                predictor='LDRF',quantile_ = None, list_of_ha_dict=None, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_LDQRF_models, \n",
    "                                predictor='LDQRF',quantile_ = 0.10, list_of_ha_dict=None, print_ = False)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_LDQRF_models, \n",
    "                                predictor='LDQRF',quantile_ = 0.25, list_of_ha_dict=None, print_ = False)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_LDQRF_models, \n",
    "                                predictor='LDQRF',quantile_ = 0.5, list_of_ha_dict=None, print_ = True)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_LDQRF_models, \n",
    "                                predictor='LDQRF',quantile_ = 0.75, list_of_ha_dict=None, print_ = False)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_LDQRF_models, \n",
    "                                predictor='LDQRF',quantile_ = 0.90, list_of_ha_dict=None, print_ = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_XGB_models, \n",
    "                                predictor='XGB',quantile_ = None, list_of_ha_dict=None, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_RF_models, \n",
    "                                predictor='RF',quantile_ = None, list_of_ha_dict=None, print_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_QRF_models, \n",
    "                                predictor='QRF',quantile_ = 0.10, list_of_ha_dict=None, print_ = False)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_QRF_models, \n",
    "                                predictor='QRF',quantile_ = 0.25, list_of_ha_dict=None, print_ = False)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_QRF_models, \n",
    "                                predictor='QRF',quantile_ = 0.5, list_of_ha_dict=None, print_ = True)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_QRF_models, \n",
    "                                predictor='QRF',quantile_ = 0.75, list_of_ha_dict=None, print_ = False)\n",
    "df_15min_ = forecasting_per_grid(df_15min_, dict_of_predictors=all_QRF_models, \n",
    "                                predictor='QRF',quantile_ = 0.90, list_of_ha_dict=None, print_ = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_.to_csv('Data/case 1/df_test_21week.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = pd.read_csv('Data/case 1/df_test_21week.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'DayofWeek', 'Hour', 'Quarter', 'OrZone', 'temp', 'wspd',\n",
       "       'prep', 'Is_Holiday', 'AR1', 'AR2', 'AR3', 'AR4', 'counts', 'zone_id',\n",
       "       'HA', 'TBATS', 'LDXGB', 'LDRF', 'LDQRF_0.1', 'LDQRF_0.25', 'LDQRF_0.5',\n",
       "       'LDQRF_0.75', 'LDQRF_0.9', 'XGB', 'RF', 'QRF_0.1', 'QRF_0.25',\n",
       "       'QRF_0.5', 'QRF_0.75', 'QRF_0.9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_15min_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'return the list of indices of rows that we will discard from df_15min_'\n",
    "# Define the condition: for example, where column 'A' is greater than 20\n",
    "condition_1 = (df_15min_['date'] == '2020-09-07')\n",
    "condition_2 = (df_15min_['Hour'] == 10) & (df_15min_['Quarter'] == 3)\n",
    "\n",
    "# Get list of indices where the condition is met\n",
    "indices_1 = df_15min_.index[condition_1].tolist()\n",
    "indices_2 = df_15min_.index[condition_2].tolist()\n",
    "indices = indices_1 + indices_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst = []\n",
    "for i in range(20):\n",
    "    df_temp = df_15min_[df_15min_['OrZone'] == f'zone {i+1}']\n",
    "    # print(len(df_temp))\n",
    "    pred_SARIMA = np.load(f'Results_Saving_Case_1/SARIMA/zone {i+1}.npy')\n",
    "    # print(len(pred_SARIMA))\n",
    "    pred_SARIMAX = np.load(f'Results_Saving_Case_1/SARIMAX/zone {i+1}.npy')\n",
    "    df_temp['SARIMA'] = pred_SARIMA\n",
    "    df_temp['SARIMAX'] = pred_SARIMAX\n",
    "    df_lst.append(df_temp)\n",
    "df_15min_ = pd.concat(df_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>OrZone</th>\n",
       "      <th>temp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>prep</th>\n",
       "      <th>Is_Holiday</th>\n",
       "      <th>AR1</th>\n",
       "      <th>...</th>\n",
       "      <th>LDQRF_0.9</th>\n",
       "      <th>XGB</th>\n",
       "      <th>RF</th>\n",
       "      <th>QRF_0.1</th>\n",
       "      <th>QRF_0.25</th>\n",
       "      <th>QRF_0.5</th>\n",
       "      <th>QRF_0.75</th>\n",
       "      <th>QRF_0.9</th>\n",
       "      <th>SARIMA</th>\n",
       "      <th>SARIMAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>zone 1</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.575611</td>\n",
       "      <td>0.664446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.758285</td>\n",
       "      <td>0.573147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>zone 1</td>\n",
       "      <td>154</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.575611</td>\n",
       "      <td>0.664446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.600581</td>\n",
       "      <td>0.477110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  DayofWeek  Hour  Quarter  OrZone  temp  wspd  prep  \\\n",
       "0   2020-09-07          0    21        1  zone 1   154    40     0   \n",
       "20  2020-09-07          0    21        2  zone 1   154    40     0   \n",
       "\n",
       "    Is_Holiday  AR1  ...  LDQRF_0.9       XGB        RF  QRF_0.1  QRF_0.25  \\\n",
       "0            0  0.0  ...        2.0  0.575611  0.664446      0.0       0.0   \n",
       "20           0  1.0  ...        2.0  0.575611  0.664446      0.0       0.0   \n",
       "\n",
       "    QRF_0.5  QRF_0.75  QRF_0.9    SARIMA   SARIMAX  \n",
       "0       1.0       1.0      2.0  0.758285  0.573147  \n",
       "20      1.0       1.0      2.0  0.600581  0.477110  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_15min_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attributes_list(df_, features_):\n",
    "    # create a list of sub-dataframes for each zone_id\n",
    "    zone_dfs = []\n",
    "    for i in range(20):\n",
    "        # TODO: ALLERT: ZONE_DF order is NOW OrZone order\n",
    "        temp = df_[df_.OrZone == f'zone {i+1}']\n",
    "        # temp = df_[df_.zone_id == i]\n",
    "        zone_dfs.append(temp[features_])\n",
    "    attributes_biglist = []\n",
    "    for j in range(len(zone_dfs[0])): # time\n",
    "        attributes_ = []\n",
    "        for z in range(len(zone_dfs)): # zone id\n",
    "            attributes_.append(zone_dfs[z].iloc[j].values)\n",
    "        attributes_biglist.append(attributes_) # list of T sublist: each sublist is from grid 1 - 20\n",
    "        # shape of attributes_biglist: (T, 20 * num_features)\n",
    "    return attributes_biglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_real_demand = create_attributes_list(df_15min_, ['counts'])\n",
    "attributes_myopic = create_attributes_list(df_15min_, ['AR1'])\n",
    "attributes_HA = create_attributes_list(df_15min_, ['HA'])\n",
    "attributes_TBATS = create_attributes_list(df_15min_, ['TBATS'])\n",
    "attributes_LDRF = create_attributes_list(df_15min_, ['LDRF'])\n",
    "attributes_RF = create_attributes_list(df_15min_, ['RF'])\n",
    "attributes_LDXGB = create_attributes_list(df_15min_, ['LDXGB'])\n",
    "attributes_XGB = create_attributes_list(df_15min_, ['XGB'])\n",
    "attributes_LDQRF = create_attributes_list(df_15min_, ['LDQRF_0.25', 'LDQRF_0.5', 'LDQRF_0.75'])\n",
    "attributes_QRF = create_attributes_list(df_15min_, [ 'QRF_0.25', 'QRF_0.5', 'QRF_0.75'])\n",
    "attributes_SARIMA = create_attributes_list(df_15min_, ['SARIMA'])\n",
    "attributes_SARIMAX = create_attributes_list(df_15min_, ['SARIMAX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_QRF_median = create_attributes_list(df_15min_, ['QRF_0.5'])\n",
    "attributes_LDQRF_median = create_attributes_list(df_15min_, ['LDQRF_0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 308\n",
      "layer 2 20\n",
      "layer 3 1\n"
     ]
    }
   ],
   "source": [
    "'layer 1: from t1 to tN, layer 2: demand val from zone 1 to zone 20, layer 3: demand val from each zone'\n",
    "print('layer 1', len(attributes_real_demand))\n",
    "print('layer 2', len(attributes_real_demand[0]))\n",
    "print('layer 3', len(attributes_real_demand[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-saved predictions, cont here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def _load_pred_dict(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_attributes_array_savedpred(model_name, test_length, \n",
    "                                      quantile_switch = False,\n",
    "                                      quantiles=(25, 75), \n",
    "                                      base_dir=\"Predictions/1_week_predictions\",\n",
    "                                      zone_order=None, dtype=np.float32, to_list=False):\n",
    "    \"\"\"\n",
    "    Returns array of shape (T, Z, P) where:\n",
    "        T = test_length\n",
    "        Z = number of zones (e.g. 20)\n",
    "        P = number of prediction variants (1 + len(quantiles) for quantile models)\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    if model_name in {'QRF', 'LDQRF'}:\n",
    "        paths.append(f\"{base_dir}/{model_name}_pred_test.pkl\")          # mean (or main) prediction\n",
    "        if quantile_switch:\n",
    "            for q in quantiles:\n",
    "                paths.append(f\"{base_dir}/{model_name}_{q}_pred_test.pkl\")  # quantile predictions\n",
    "    else:\n",
    "        paths.append(f\"{base_dir}/{model_name}_pred_test.pkl\")\n",
    "\n",
    "    # Load all prediction dicts (cached) â€“ each: {zone_name: array_like (len >= test_length)}\n",
    "    pred_dicts = [_load_pred_dict(p) for p in paths]\n",
    "\n",
    "    # Establish consistent zone order\n",
    "    if zone_order is None:\n",
    "        # Use intersection order from first dict; sort for determinism\n",
    "        zone_order = [f'zone {i}' for i in range(1, 21)]\n",
    "\n",
    "    Z = len(zone_order)\n",
    "    P = len(pred_dicts)\n",
    "    T = test_length\n",
    "\n",
    "    # Preallocate (P, Z, T)\n",
    "    cube = np.empty((P, Z, T), dtype=dtype)\n",
    "    for p_idx, dct in enumerate(pred_dicts):\n",
    "        # Optionally validate keys once\n",
    "        # Fill per zone (vectorized over time)\n",
    "        for z_idx, z in enumerate(zone_order):\n",
    "            arr = np.asarray(dct[z])\n",
    "            if arr.shape[0] < T:\n",
    "                raise ValueError(f\"Zone {z} has only {arr.shape[0]} preds (< {T}).\")\n",
    "            cube[p_idx, z_idx, :] = arr[:T]\n",
    "\n",
    "    # Reorder axes to (T, Z, P)\n",
    "    attributes = np.transpose(cube, (2, 1, 0)) # (T, Z, P)\n",
    "\n",
    "    if to_list:\n",
    "        return attributes.tolist(), zone_order\n",
    "    return attributes, zone_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 20, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_LSTM, _ = create_attributes_array_savedpred('LSTM', 307, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "first_row = attributes_LSTM[0:1].copy()          # shape (1, 20, 1)\n",
    "attributes_LSTM = np.concatenate([first_row, attributes_LSTM], axis=0)\n",
    "\n",
    "attributes_LSTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_SARIMA, _ = create_attributes_array_savedpred('SARIMA', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_SARIMAX, _ = create_attributes_array_savedpred('SARIMAX', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_TBATS, zone_order = create_attributes_array_savedpred('TBATS', 308,\n",
    "                            base_dir=\"Predictions/1_week_predictions\",\n",
    "                            zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "# attributes_LSTM, _ = create_attributes_array_savedpred('LSTM', 308, quantiles=(25, 75),\n",
    "#                         base_dir=\"Predictions/1_week_predictions\",\n",
    "#                         zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_RF, _ = create_attributes_array_savedpred('RF', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDRF, _ = create_attributes_array_savedpred('LDRF', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_XGB, _ = create_attributes_array_savedpred('XGB', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDXGB, _ = create_attributes_array_savedpred('LDXGB', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_QRF_median, _ = create_attributes_array_savedpred('QRF', 308, quantiles=(25,75),\n",
    "                                                             base_dir=\"Predictions/1_week_predictions\",\n",
    "                                                             zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDQRF_median, _ = create_attributes_array_savedpred('LDQRF', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_LDQRF, _ = create_attributes_array_savedpred('LDQRF', 308, quantiles=(25, 75),\n",
    "                                                     quantile_switch = True,\n",
    "                                                    base_dir=\"Predictions/1_week_predictions\",\n",
    "                                                    zone_order=None, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_QRF, _ = create_attributes_array_savedpred('QRF', 308, quantiles=(25, 75),quantile_switch = True,\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=None, dtype=np.float32, to_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 20, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_LDQRF_median.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 20, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_QRF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def calculate_last_merge_distance(clustering, distance_matrix, num_current_cluster_):\n",
    "  n_samples = distance_matrix.shape[0]\n",
    "  # Retrieve the children attribute\n",
    "  children = clustering.children_\n",
    "\n",
    "  # Function to compute average linkage distance\n",
    "  def compute_avg_linkage_dist(i, j, dist_matrix):\n",
    "      cluster_i_points = get_cluster_points(i, children, n_samples)\n",
    "      cluster_j_points = get_cluster_points(j, children, n_samples)\n",
    "      return np.mean([dist_matrix[p1, p2] for p1 in cluster_i_points for p2 in cluster_j_points])\n",
    "\n",
    "  # Helper function to get all points in a cluster\n",
    "  def get_cluster_points(cluster_idx, children, n_samples):\n",
    "      if cluster_idx < n_samples:\n",
    "          return [cluster_idx]\n",
    "      else:\n",
    "          cluster = []\n",
    "          for child in children[cluster_idx - n_samples]:\n",
    "              cluster.extend(get_cluster_points(child, children, n_samples))\n",
    "          return cluster\n",
    "\n",
    "  # Compute the distances of each merge\n",
    "  merge_distances = []\n",
    "  for i, (child1, child2) in enumerate(children):\n",
    "      dist = compute_avg_linkage_dist(child1, child2, distance_matrix)\n",
    "      merge_distances.append(dist)\n",
    "\n",
    "  # Get the distance of the last merge before termination\n",
    "  if merge_distances:\n",
    "      last_merge_distance = merge_distances[-(num_current_cluster_ - 1)]\n",
    "      return last_merge_distance\n",
    "  else:\n",
    "      return None\n",
    "\n",
    "# network of pickup zones\n",
    "def create_zone_graph(edges, num_nodes=20):\n",
    "    G = nx.Graph()\n",
    "    for node in range(num_nodes):\n",
    "        G.add_node(node)\n",
    "    for edge in edges:\n",
    "        G.add_edge(*edge)\n",
    "    return G\n",
    "\n",
    "def get_neighbors(G_general):\n",
    "    return {node: list(G_general.neighbors(node)) for node in G_general.nodes}\n",
    "\n",
    "def create_cluster_graph(G_zone, cluster_list):\n",
    "    # cluster_list: [zone ji] for all zones i belong to cluster j\n",
    "    G_cluster = nx.Graph()\n",
    "    for j in range(len(cluster_list)):\n",
    "        G_cluster.add_node(j)\n",
    "    for i, cluster1 in enumerate(cluster_list):\n",
    "        for j, cluster2 in enumerate(cluster_list):\n",
    "            if cluster1 != cluster2 and any(G_zone.has_edge(node1, node2) for node1 in cluster1 for node2 in cluster2):\n",
    "                G_cluster.add_edge(i, j)\n",
    "    return G_cluster\n",
    "\n",
    "def get_inner_neighbors(num_zones, cluster_list):\n",
    "    # cluster_list: [zone ji] for all zones i belong to cluster j\n",
    "    inner_neighbors = {}\n",
    "    for i in range(num_zones):\n",
    "        for cluster in cluster_list:\n",
    "            if i in cluster:\n",
    "                inner_neighbors[i] = cluster.copy()\n",
    "                inner_neighbors[i].remove(i)\n",
    "    return inner_neighbors\n",
    "\n",
    "def get_outer_neighbors(num_zones, max_cluster_size, cluster_list, G_cluster):\n",
    "    cluster_outer_neighbor_dict = get_neighbors(G_cluster)\n",
    "    zone_outer_neighbor_dict = {}\n",
    "\n",
    "    # Create a new list that only includes clusters with size less than or equal to max_cluster_size\n",
    "    valid_clusters = [cluster for cluster in cluster_list if len(cluster) < max_cluster_size]\n",
    "\n",
    "    # Update cluster_outer_neighbor_dict to only include valid clusters\n",
    "    cluster_outer_neighbor_dict = {i: cluster_outer_neighbor_dict[i] for i in range(len(valid_clusters))}\n",
    "\n",
    "    for i in range(num_zones):\n",
    "        neighbors_ = []\n",
    "        for c_id, cluster in enumerate(valid_clusters):\n",
    "            if i in cluster:\n",
    "                cluster_neighbors = cluster_outer_neighbor_dict[c_id]\n",
    "                for nb_cluster_id in cluster_neighbors:\n",
    "                    if nb_cluster_id in cluster_outer_neighbor_dict.keys():\n",
    "                        if len(valid_clusters[nb_cluster_id]) + len(cluster) <= max_cluster_size:\n",
    "                            neighbors_ += valid_clusters[nb_cluster_id]\n",
    "        zone_outer_neighbor_dict[i] = list(set(neighbors_))\n",
    "\n",
    "    return zone_outer_neighbor_dict\n",
    "\n",
    "def create_linkage_matrix(num_zones, zone_outer_neighbor_dict, zone_inner_neighbor_dict):\n",
    "    # create the potential linkage matrix\n",
    "    potential_linkages = np.zeros((num_zones, num_zones))\n",
    "    for zone, outer_neighbors in zone_outer_neighbor_dict.items():\n",
    "        for outer_neighbor in outer_neighbors:\n",
    "            potential_linkages[zone][outer_neighbor] = 1\n",
    "    assert np.allclose(potential_linkages, potential_linkages.T, rtol=1e-05, atol=1e-08)\n",
    "    assert np.allclose(np.diag(potential_linkages), np.zeros(num_zones), rtol=1e-05, atol=1e-08)\n",
    "\n",
    "    # create the must linkage matrix\n",
    "    must_linkages = np.zeros((num_zones, num_zones))\n",
    "    for zone, inner_neighbors in zone_inner_neighbor_dict.items():\n",
    "        for inner_neighbor in inner_neighbors:\n",
    "            must_linkages[zone][inner_neighbor] = 1\n",
    "    # the diagonal of the must linkage matrix should be zero, and the matrix should be symmetric\n",
    "    assert np.allclose(must_linkages, must_linkages.T, rtol=1e-05, atol=1e-08)\n",
    "    assert np.allclose(np.diag(must_linkages), np.zeros(num_zones), rtol=1e-05, atol=1e-08)\n",
    "\n",
    "    return potential_linkages, must_linkages\n",
    "\n",
    "# TODO: implement the weighted Euclidean distance in main code\n",
    "def weighted_euclidean_distance(v1, v2, w=np.array([1,1,1])):\n",
    "    # Calculate the weighted Euclidean distance\n",
    "    diff = v1 - v2\n",
    "    weighted_diff = w * (diff ** 2)\n",
    "    weighted_distance = np.sqrt(np.sum(weighted_diff))\n",
    "    return weighted_distance\n",
    "\n",
    "def get_distance_metrix(num_zones, attributes_, potential_linkages, must_linkages, attribute_type = 'single'):\n",
    "    # attributes_ = df[feature_list], a 2D array of the features of all zones, for specific time step t\n",
    "    # attributes_[i]:  the feature vector of zone i\n",
    "    distance_matrix = np.zeros((num_zones, num_zones))\n",
    "    for i in range(num_zones):\n",
    "        for j in range(num_zones):\n",
    "            if i != j:\n",
    "                if attribute_type == 'single':\n",
    "                    distance_matrix[i, j] = np.linalg.norm(attributes_[i] - attributes_[j])\n",
    "                elif attribute_type == 'multiple':\n",
    "                    # distance_matrix[i, j] = weighted_euclidean_distance(attributes_[i], attributes_[j], w=np.array([1,2,1]))\n",
    "                    distance_matrix[i, j] = weighted_euclidean_distance(attributes_[i], attributes_[j], w=np.array([1,1,1]))\n",
    "                if potential_linkages[i, j] == 0:\n",
    "                    # if two zones are impossible to link, set their distance to a large number\n",
    "                    distance_matrix[i, j] = 99\n",
    "                if must_linkages[i, j] == 1:\n",
    "                    # if two zones must be linked, set their distance to zero\n",
    "                    distance_matrix[i, j] = 0\n",
    "    \n",
    "    # Ensure the distance matrix is symmetric\n",
    "    distance_matrix = np.minimum(distance_matrix, distance_matrix.T)\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "def contingency_constrained_hierarchical_clustering(attributes,\n",
    "                                                    zone_edges,\n",
    "                                                    zone_actual_demand, \n",
    "                                                    zone_pred_demand,\n",
    "                                                    num_zones = 20,\n",
    "                                                    ultimate_num_clusters = 5, \n",
    "                                                    max_cluster_size = 6, \n",
    "                                                    distance_threshold_=9,\n",
    "                                                    distance_measure = 'ward',\n",
    "                                                    print_ = False,\n",
    "                                                    attribute_type = 'single'):\n",
    "    '''\n",
    "    This function generates the clustering of the zones based on their predicted demand values,\n",
    "    by the contingency constrained hierarchical clustering process\n",
    "\n",
    "    Inputs: \n",
    "    df: the dataframe containing the predicted and previous demand values of all pick-up zones, for each time step t\n",
    "    feature_list: the list of features that will be used as criterion of clustering\n",
    "\n",
    "    Constraints:\n",
    "    max_num_clusters: the maximum number of clusters that can be generated, default = 5\n",
    "    min_num_clusters: the minimum number of clusters that can be generated, default = 2\n",
    "    max_cluster_size: the maximum number of zones that can be in a cluster, default = 9\n",
    "    min_cluster_size: the minimum number of zones that can be in a cluster, default = 1\n",
    "    (contingency constraints are updated in the linkage matrix, during the clustering process)\n",
    "    distance_measure: the distance measure used in the clustering process, default = 'ward', we should perform sensitivity analysis over this parameter\n",
    "\n",
    "    Outputs:\n",
    "    clustering.labels_: the cluster label of each zone\n",
    "    cluster_demand_actual: the average demand value of each cluster, calculated using actual demand values, based on the clustering result over predicted attributes\n",
    "    cluster_demand_pred: the average predicted demand value of each cluster, calculated using predicted demand values, based on the clustering result over predicted attributes\n",
    "    '''\n",
    "    # initialize helper variables\n",
    "    cluster_list= [[i] for i in range(num_zones)] # [zone ji] for all zones i belong to cluster j\n",
    "    # print(cluster_list)\n",
    "    cluster_sizes = np.ones(num_zones)\n",
    "    # temp_max_cluster_size = np.max(cluster_sizes)\n",
    "    last_merge_distance_ = 99\n",
    "\n",
    "    # intialize the network of zones & clusters using networkx\n",
    "    G_zone = create_zone_graph(zone_edges)\n",
    "    # zone_geoconnected_dict = get_neighbors(G_zone)\n",
    "\n",
    "    # Perform hierarchical clustering: \n",
    "    num_current_cluster_ = num_zones\n",
    "    violations_ = False\n",
    "    \n",
    "    while num_current_cluster_ > ultimate_num_clusters and violations_ == False:\n",
    "        num_current_cluster_ -= 1\n",
    "        # print(f'Current number of clusters: {num_current_cluster_}')\n",
    "        # create the cluster graph\n",
    "        G_cluster = create_cluster_graph(G_zone, cluster_list)\n",
    "        zone_outer_neighbor_dict = get_outer_neighbors(num_zones, max_cluster_size, cluster_list, G_cluster) # this is used to update the potential linkage matrix\n",
    "        zone_inner_neighbor_dict = get_inner_neighbors(num_zones, cluster_list) # this is used to update the must linkage matrix\n",
    "\n",
    "        # how to create the linkage matrix from zone_outer_neighbor_dict and zone_inner_neighbor_dict\n",
    "        potential_linkages, must_linkages = create_linkage_matrix(num_zones, zone_outer_neighbor_dict, zone_inner_neighbor_dict)\n",
    "\n",
    "        # Modify the distance matrix to enforce can-link constraints\n",
    "        distance_matrix = get_distance_metrix(num_zones, attributes, potential_linkages, must_linkages, attribute_type)\n",
    "        # print('--The next smallest distance in line', linkage(distance_matrix, method='average'))\n",
    "        \n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=num_current_cluster_,\n",
    "            affinity='precomputed', # we use the precomputed distance matrix\n",
    "            linkage=distance_measure,\n",
    "            compute_full_tree=False\n",
    "        )\n",
    "\n",
    "        # Fit the clustering\n",
    "        clustering.fit(distance_matrix)\n",
    "\n",
    "        # size of each cluster after the clustering\n",
    "        cluster_sizes = np.zeros(num_zones)\n",
    "        for i in range(num_zones):\n",
    "            cluster_sizes[clustering.labels_[i]] += 1\n",
    "\n",
    "        # cluster similarity threshold via last merge distance\n",
    "        last_merge_distance = calculate_last_merge_distance(clustering, distance_matrix, num_current_cluster_)\n",
    "        # if last_merge_distance > 10:\n",
    "        #     print('last merge distance', last_merge_distance)\n",
    "        #     print(cluster_sizes)\n",
    "        if last_merge_distance > distance_threshold_: \n",
    "            violations_ = True\n",
    "\n",
    "        # create cluster list, which is a 2D list of zones in each cluster\n",
    "        cluster_list = [[] for _ in range(num_current_cluster_)]\n",
    "        for i in range(num_zones):\n",
    "            cluster_list[clustering.labels_[i]].append(i)\n",
    "\n",
    "        # check whether zones that do not included by zone_inner_neighbor_dict and zone_outer_neighbor_dict are in the same cluster\n",
    "        for i in range(num_zones):\n",
    "            if violations_ == False:\n",
    "                # get the list of zones in the same cluster with zone i\n",
    "                cluster_i = cluster_list[clustering.labels_[i]]\n",
    "                # check whether any zone j in cluster_i is neither in zone_inner_neighbor_dict nor in zone_outer_neighbor_dict\n",
    "                for j in cluster_i:\n",
    "                    if j != i: \n",
    "                        if j not in zone_inner_neighbor_dict[i] and j not in zone_outer_neighbor_dict[i]:\n",
    "                            violations_ = True\n",
    "                            break\n",
    "\n",
    "        # revert to the previous clustering\n",
    "        if violations_:\n",
    "            num_current_cluster_ += 1\n",
    "            if print_:\n",
    "                print('Contingency violation detected, reverting to the previous clustering')\n",
    "            clustering = AgglomerativeClustering(n_clusters=num_current_cluster_,\n",
    "                                                affinity='precomputed', # we use the precomputed distance matrix\n",
    "                                                linkage=distance_measure,\n",
    "                                                compute_full_tree=False)\n",
    "\n",
    "            # Fit the clustering\n",
    "            clustering.fit(distance_matrix)\n",
    "\n",
    "    # size of each cluster after the clustering\n",
    "    cluster_sizes = np.zeros(num_current_cluster_)\n",
    "    for i in range(num_zones):\n",
    "        cluster_sizes[clustering.labels_[i]] += 1\n",
    "\n",
    "    cluster_list = [[] for _ in range(num_current_cluster_)]\n",
    "    for i in range(num_zones):\n",
    "        cluster_list[clustering.labels_[i]].append(i)\n",
    "\n",
    "    # for each cluster, calculate the average demand value using zone_actual_demand\n",
    "    cluster_medi_demand_actual = []\n",
    "    cluster_medi_demand_pred = []\n",
    "    # cluster_avg_demand_actual = []\n",
    "    # cluster_avg_demand_pred = []\n",
    "    for cluster in cluster_list:\n",
    "        # demand = 0\n",
    "        # demand_pred = 0\n",
    "        demand = []\n",
    "        demand_pred = []\n",
    "        for zone in cluster:\n",
    "            # demand += zone_actual_demand[zone]\n",
    "            # demand_pred += zone_pred_demand[zone]\n",
    "            demand.append(zone_actual_demand[zone])\n",
    "            demand_pred.append(zone_pred_demand[zone])\n",
    "        # cluster_avg_demand_actual.append(demand / len(cluster))\n",
    "        # cluster_avg_demand_pred.append(demand_pred / len(cluster))\n",
    "        cluster_medi_demand_actual.append(np.median(demand))\n",
    "        cluster_medi_demand_pred.append(np.median(demand_pred))\n",
    "    \n",
    "    if print_:\n",
    "        print('number of clusters', num_current_cluster_)\n",
    "        print('final labels', clustering.labels_)\n",
    "        print('final cluster sizes', cluster_sizes)\n",
    "        # print('cluster average demand (actual)', cluster_avg_demand_actual)\n",
    "        # print('cluster average demand (predict)', cluster_avg_demand_pred)\n",
    "        print('cluster median demand (actual)', cluster_medi_demand_actual)\n",
    "        print('cluster median demand (predict)', cluster_medi_demand_pred)\n",
    "\n",
    "    # create an array that contains the cluster average demand for each zone\n",
    "    cluster_demand_actual = np.zeros(num_zones)\n",
    "    cluster_demand_pred = np.zeros(num_zones)\n",
    "    for i in range(num_zones):\n",
    "        # print('zone', i, 'cluster', clustering.labels_[i])\n",
    "        # cluster_demand_actual[i] = cluster_avg_demand_actual[clustering.labels_[i]]\n",
    "        # cluster_demand_pred[i] = cluster_avg_demand_pred[clustering.labels_[i]]\n",
    "        cluster_demand_actual[i] = cluster_medi_demand_actual[clustering.labels_[i]]\n",
    "        cluster_demand_pred[i] = cluster_medi_demand_pred[clustering.labels_[i]]\n",
    "    \n",
    "    return clustering.labels_, cluster_demand_actual, cluster_demand_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier-Clustering with real demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def hier_clustering_per_timestep(attributes_biglist,\n",
    "                                 zone_wise_demand_actual,\n",
    "                                 zone_wise_demand_pred, \n",
    "                                 zone_edges_,\n",
    "                                 min_num_cluster = 2,\n",
    "                                 max_size_per_cluster = 7,\n",
    "                                 distance_threshold_ = 9,\n",
    "                                 proximacy_measure = 'average',\n",
    "                                 attribute_type = 'single'):\n",
    "    # # ID order\n",
    "    # zone_edges_ = [(1,3), (1,4), (1,6), (2,5), (2,13), (3,6), (4,6), (4,5),\n",
    "    #                (5,13), (7,11), (8,10), (8,9), (9,10), (9,11), (9,14),\n",
    "    #                (10,14), (10,18), (11,14), (12,14), (12,13), (14,18),\n",
    "    #                (15,19), (16,17), (16,18), (16,19),(17,18)]\n",
    "    num_timestep = len(attributes_biglist)\n",
    "    list_of_labels = []\n",
    "    list_of_cluster_demand_actual = []\n",
    "    list_of_cluster_demand_pred = []\n",
    "    # new for computational time recording\n",
    "    start_time = time.time()\n",
    "    for i in range(num_timestep):\n",
    "        clustering_labels_, cluster_demand_actual, cluster_demand_pred =contingency_constrained_hierarchical_clustering(\n",
    "                                                attributes = attributes_biglist[i],\n",
    "                                                zone_edges = zone_edges_,\n",
    "                                                zone_actual_demand = zone_wise_demand_actual[i], \n",
    "                                                zone_pred_demand = zone_wise_demand_pred[i],\n",
    "                                                num_zones = 20,\n",
    "                                                distance_threshold_ = distance_threshold_,\n",
    "                                                ultimate_num_clusters = min_num_cluster, \n",
    "                                                max_cluster_size = max_size_per_cluster, \n",
    "                                                distance_measure = proximacy_measure,\n",
    "                                                attribute_type = attribute_type)\n",
    "        # COLLECT all the labels, cluster_actual and cluster_pred into a full length array\n",
    "        list_of_labels.append(clustering_labels_) # at timestep t and for all 20 grids\n",
    "        list_of_cluster_demand_actual.append(cluster_demand_actual)\n",
    "        list_of_cluster_demand_pred.append(cluster_demand_pred)\n",
    "    # print the time elapsed for each clustering\n",
    "    print(f'Average Time elapsed for clustering at time step {i}: {(time.time() - start_time)/num_timestep:.3f} seconds')\n",
    "\n",
    "    list_of_labels = np.array(list_of_labels).flatten()\n",
    "    list_of_cluster_demand_actual = np.array(list_of_cluster_demand_actual).flatten()\n",
    "    list_of_cluster_demand_pred = np.array(list_of_cluster_demand_pred).flatten()\n",
    "    return list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred\n",
    "\n",
    "def clustering_evaluation(df_15min, clustering_labels, cluster_demand_actual, cluster_demand_pred):\n",
    "    '''\n",
    "    This universal evaluation func compares the estimated courier resource needed post clustering\n",
    "    \n",
    "    Measure: Error/Difference from {estimated per cluster resource (using actual demand values attributes) - estimated per cluster resource (using pred demand attributes)}\n",
    "\n",
    "    Note that, the measure accounts for influence from\n",
    "    - (1) forecasting error from the point predicted demand for each zone per 15min time window\n",
    "    - (2) the clustering deviation caused by using (potentially flawed) predicted demand rather than perfect predictions (the actual demand)\n",
    "    - * the deterministic predictions of Quantile Regression Forest are should be more or less the same as those generated from Random Forest\n",
    "    - * as they follow the very same tree regressor generation criterion\n",
    "\n",
    "    '''\n",
    "    # create df_full via arranging df by date, hour, quarter, zone_id\n",
    "    df_full = df_15min.sort_values(by=['date', 'Hour', 'Quarter', 'zone_id'])\n",
    "    df_full['cluster_label'] = clustering_labels\n",
    "    df_full['cluster_demand_actual'] = cluster_demand_actual\n",
    "    df_full['cluster_demand_pred'] = cluster_demand_pred\n",
    "\n",
    "    # calculate the error/difference between the actual and predicted demand for each cluster\n",
    "    zone_wise_MAE = np.zeros(20)\n",
    "    zone_wise_RMSE = np.zeros(20)\n",
    "    zone_wise_RMSLE = np.zeros(20)\n",
    "    zone_wise_residual = np.zeros(20)\n",
    "\n",
    "    for i in range(20):\n",
    "        zone_name = f'zone {i+1}'\n",
    "        temp = df_full[df_full.OrZone == zone_name]\n",
    "        mae, rmse, rmsle, resid_std = prediction_evaluate(temp.cluster_demand_pred.values, temp.cluster_demand_actual.values)\n",
    "        zone_wise_MAE[i] = mae\n",
    "        zone_wise_RMSE[i] = rmse\n",
    "        zone_wise_RMSLE[i] = rmsle\n",
    "        zone_wise_residual[i] = resid_std\n",
    "    \n",
    "    # print the evaluation results\n",
    "    print(f'MAE: {np.mean(zone_wise_MAE):.3f}({np.std(zone_wise_MAE):.3f})')\n",
    "    print(f'RMSE: {np.mean(zone_wise_RMSE):.3f}({np.std(zone_wise_RMSE):.3f})')\n",
    "    print(f'RMSLE: {np.mean(zone_wise_RMSLE):.3f}({np.std(zone_wise_RMSLE):.3f})')\n",
    "    print(f'Residual STD: {np.mean(zone_wise_residual):.3f}({np.std(zone_wise_residual):.3f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCHC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_outcomes = pd.DataFrame()\n",
    "\n",
    "global_distance_threshold = 9 \n",
    "# the threshold used to determine the distance between zones in the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_real_demand, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "cluster_outcomes['actual_labels'] = list_of_labels\n",
    "cluster_outcomes['actual_cluster_demand'] = list_of_cluster_demand_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_real_demand, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, SARIMA_list_of_cluster_demand_actual, SARIMA_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_SARIMA,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_SARIMA, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMA_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMA_list_of_cluster_demand_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, SARIMAX_list_of_cluster_demand_actual, SARIMAX_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_SARIMAX,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_SARIMAX, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMAX_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMAX_list_of_cluster_demand_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, TBATS_list_of_cluster_demand_actual, TBATS_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_TBATS,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_TBATS, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, TBATS_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, TBATS_list_of_cluster_demand_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, LSTM_list_of_cluster_demand_actual, LSTM_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_LSTM,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LSTM, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LSTM_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LSTM_list_of_cluster_demand_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, RF_list_of_cluster_demand_actual, RF_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_RF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_RF, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, RF_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, RF_list_of_cluster_demand_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, LDRF_list_of_cluster_demand_actual, LDRF_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_LDRF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDRF, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            distance_threshold_ = global_distance_threshold,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDRF_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDRF_list_of_cluster_demand_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, XGB_list_of_cluster_demand_actual, XGB_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_XGB,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_XGB, \n",
    "                                                                                                            zone_edges_=connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, XGB_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, XGB_list_of_cluster_demand_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, LDXGB_list_of_cluster_demand_actual, LDXGB_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_LDXGB,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDXGB, \n",
    "                                                                                                            zone_edges_= connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDXGB_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDXGB_list_of_cluster_demand_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Median/Single QRF 21 week'\n",
    "list_of_labels, medi_QRF_list_of_cluster_demand_actual, medi_QRF_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_QRF_median,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_QRF_median, \n",
    "                                                                                                            zone_edges_= connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average',\n",
    "                                                                                                            attribute_type = 'single')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_QRF_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_QRF_list_of_cluster_demand_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Quantile_QRF 21 week'\n",
    "list_of_labels, QRF_list_of_cluster_demand_actual, QRF_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_QRF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_QRF_median, \n",
    "                                                                                                            zone_edges_= connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average',\n",
    "                                                                                                            attribute_type = 'multiple')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, QRF_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, QRF_list_of_cluster_demand_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Median/Single LDQRF 21 week'\n",
    "list_of_labels, medi_LDQRF_list_of_cluster_demand_actual, medi_LDQRF_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_LDQRF_median,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDQRF_median, \n",
    "                                                                                                            zone_edges_= connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average',\n",
    "                                                                                                            attribute_type = 'single')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_LDQRF_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_LDQRF_list_of_cluster_demand_actual)\n",
    "\n",
    "cluster_outcomes['medi_LDQRF_labels'] = list_of_labels\n",
    "cluster_outcomes['medi_LDQRF_cluster_demand'] = medi_LDQRF_list_of_cluster_demand_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Quantile LDQRF 21 week'\n",
    "list_of_labels, LDQRF_list_of_cluster_demand_actual, LDQRF_list_of_cluster_demand_pred = hier_clustering_per_timestep(attributes_biglist = attributes_LDQRF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDQRF_median, \n",
    "                                                                                                            zone_edges_= connected_pairs,\n",
    "                                                                                                            min_num_cluster = 3,\n",
    "                                                                                                            max_size_per_cluster = 9,\n",
    "                                                                                                            # max_size_per_cluster = 7,\n",
    "                                                                                                            proximacy_measure = 'average',\n",
    "                                                                                                            attribute_type = 'multiple')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDQRF_list_of_cluster_demand_pred)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDQRF_list_of_cluster_demand_actual)\n",
    "\n",
    "cluster_outcomes['Quan_LDQRF_labels'] = list_of_labels\n",
    "cluster_outcomes['Quan_LDQRF_cluster_demand'] = LDQRF_list_of_cluster_demand_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CKMC_clustering(attributes, attribute_type, zone_dict,zone_actual_demand,zone_pred_demand):\n",
    "    rows = []\n",
    "\n",
    "    if attribute_type in ['QRF','LDQRF']:\n",
    "        for j,z in enumerate(list(zone_dict.keys())[:20]): # follow the order of\n",
    "            address = zone_dict[z]\n",
    "            combined_pred = 1*attributes[j][0] + 2* attributes[j][1] + 1*attributes[j][2]\n",
    "            rows.append({'prediction':combined_pred,\n",
    "                        'latitude':address[0],\n",
    "                        'longitude':address[1]})\n",
    "    else:\n",
    "        for j,z in enumerate(list(zone_dict.keys())[:20]): # follow the order of zone_dict keys zone 1, 2, ... 20\n",
    "            address = zone_dict[z]\n",
    "            rows.append({'prediction':attributes[j],\n",
    "                        'latitude':address[0],\n",
    "                        'longitude':address[1]})\n",
    "\n",
    "    pred_dict = pd.DataFrame(rows, columns=['prediction','latitude','longitude'])\n",
    "\n",
    "    cluster_labels, _, _ = clustering(data_normalization(pred_dict), min_k=3, max_k=6, cluster_method='constrained_k-means')\n",
    "    \n",
    "    num_current_cluster_ = len(set(cluster_labels))\n",
    "    num_zones = len(attributes)\n",
    "    # add each zone to its corresponding cluster\n",
    "    cluster_list = [[] for _ in range(num_current_cluster_)]\n",
    "    for i in range(num_zones):\n",
    "        cluster_list[cluster_labels[i]].append(i)\n",
    "    \n",
    "    # for each cluster, calculate the average demand value using zone_actual_demand\n",
    "    cluster_medi_demand_actual = []\n",
    "    cluster_medi_demand_pred = []\n",
    "\n",
    "    for cluster in cluster_list:\n",
    "        demand = []\n",
    "        demand_pred = []\n",
    "        for zone in cluster:\n",
    "            demand.append(zone_actual_demand[zone])\n",
    "            demand_pred.append(zone_pred_demand[zone])\n",
    "        cluster_medi_demand_actual.append(np.median(demand))\n",
    "        cluster_medi_demand_pred.append(np.median(demand_pred))\n",
    "    \n",
    "    # create an array that contains the cluster average demand for each zone\n",
    "    cluster_demand_actual = np.zeros(num_zones)\n",
    "    cluster_demand_pred = np.zeros(num_zones)\n",
    "    for i in range(num_zones):\n",
    "        cluster_demand_actual[i] = cluster_medi_demand_actual[cluster_labels[i]]\n",
    "        cluster_demand_pred[i] = cluster_medi_demand_pred[cluster_labels[i]]\n",
    "\n",
    "    return cluster_labels, cluster_demand_actual, cluster_demand_pred\n",
    "\n",
    "\n",
    "\n",
    "def CKMC_per_timestep(attributes_biglist,\n",
    "                      zone_wise_demand_actual,\n",
    "                      zone_wise_demand_pred, \n",
    "                      zone_dict,\n",
    "                      method='constrained_k-means',\n",
    "                      prediction_type='actual'):\n",
    "\n",
    "    # DEBUGGER check attribute_type\n",
    "    print(f'Attribute type: {prediction_type}')\n",
    "    print('check input attribute shape', attributes_biglist.shape)\n",
    "    print('check input zone_wise_demand_pred shape', zone_wise_demand_pred.shape)\n",
    "\n",
    "    num_timestep = attributes_biglist.shape[0]\n",
    "    list_of_labels = []\n",
    "    list_of_cluster_demand_actual = []\n",
    "    list_of_cluster_demand_pred = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    # new for computational time recording\n",
    "    for i in tqdm(range(num_timestep)):\n",
    "        cluster_labels, cluster_demand_actual, cluster_demand_pred = CKMC_clustering(attributes_biglist[i], \n",
    "                                 prediction_type, \n",
    "                                 zone_dict,\n",
    "                                 zone_wise_demand_actual[i],\n",
    "                                 zone_wise_demand_pred[i])\n",
    "        list_of_labels.append(cluster_labels)\n",
    "        list_of_cluster_demand_actual.append(cluster_demand_actual)\n",
    "        list_of_cluster_demand_pred.append(cluster_demand_pred)\n",
    "    print(f'Average Time elapsed for clustering at time step {i}: {(time.time() - start_time)/num_timestep:.3f} seconds')\n",
    "    \n",
    "    list_of_labels = np.array(list_of_labels).flatten()\n",
    "    list_of_cluster_demand_actual = np.array(list_of_cluster_demand_actual).flatten()\n",
    "    list_of_cluster_demand_pred = np.array(list_of_cluster_demand_pred).flatten()\n",
    "\n",
    "    return list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_ = pd.read_csv('Data/EU_use_case/df_test_21week.csv')\n",
    "zone_id_dict = {'zone 1':1, 'zone 2': 2, 'zone 3': 3, 'zone 4': 4, 'zone 5': 5, 'zone 6': 6, 'zone 7': 7, 'zone 8': 8, 'zone 9': 9, 'zone 10': 10,\n",
    "                'zone 11': 11, 'zone 12': 12, 'zone 13': 13, 'zone 14': 14, 'zone 15': 15, 'zone 16': 16, 'zone 17': 17, 'zone 18': 18, 'zone 19': 19, 'zone 20': 20}\n",
    "df_15min_['zone_order'] = df_15min_['OrZone'].map(zone_id_dict)\n",
    "df_15min_ = df_15min_.sort_values(by=['date', 'Hour', 'Quarter', 'zone_order']).reset_index(drop=True)\n",
    "del df_15min_['zone_id']\n",
    "zone_order = df_15min_['OrZone'].unique()\n",
    "\n",
    "# load zone address from txt file\n",
    "zone_addresses = None\n",
    "\n",
    "df_15min_['lateitude'] = df_15min_['OrZone'].map(lambda x: zone_addresses[x][0])\n",
    "df_15min_['longitude'] = df_15min_['OrZone'].map(lambda x: zone_addresses[x][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def _load_pred_dict(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_attributes_array_savedpred(model_name, test_length, \n",
    "                                      quantile_switch = False,\n",
    "                                      quantiles=(25, 75), \n",
    "                                      base_dir=\"Predictions/1_week_predictions\",\n",
    "                                      zone_order=None, dtype=np.float32, to_list=False):\n",
    "    \"\"\"\n",
    "    Returns array of shape (T, Z, P) where:\n",
    "        T = test_length\n",
    "        Z = number of zones (e.g. 20)\n",
    "        P = number of prediction variants (1 + len(quantiles) for quantile models)\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    if model_name in {'QRF', 'LDQRF'}:\n",
    "        paths.append(f\"{base_dir}/{model_name}_pred_test.pkl\")          # mean (or main) prediction\n",
    "        if quantile_switch:\n",
    "            for q in quantiles:\n",
    "                paths.append(f\"{base_dir}/{model_name}_{q}_pred_test.pkl\")  # quantile predictions\n",
    "    elif model_name == 'actual':\n",
    "        paths.append(f\"{base_dir}/LDRF_actual_test.pkl\")\n",
    "    else:\n",
    "        paths.append(f\"{base_dir}/{model_name}_pred_test.pkl\")\n",
    "\n",
    "    # Load all prediction dicts (cached) â€“ each: {zone_name: array_like (len >= test_length)}\n",
    "    pred_dicts = [_load_pred_dict(p) for p in paths]\n",
    "\n",
    "    # Establish consistent zone order\n",
    "    if zone_order is None:\n",
    "        # Use intersection order from first dict; sort for determinism\n",
    "        zone_order = [f'zone {i}' for i in range(1, 21)]\n",
    "\n",
    "    Z = len(zone_order)\n",
    "    P = len(pred_dicts)\n",
    "    T = test_length\n",
    "\n",
    "    # Preallocate (P, Z, T)\n",
    "    cube = np.empty((P, Z, T), dtype=dtype)\n",
    "    for p_idx, dct in enumerate(pred_dicts):\n",
    "        # Optionally validate keys once\n",
    "        # Fill per zone (vectorized over time)\n",
    "        for z_idx, z in enumerate(zone_order):\n",
    "            arr = np.asarray(dct[z])\n",
    "            if arr.shape[0] < T:\n",
    "                raise ValueError(f\"Zone {z} has only {arr.shape[0]} preds (< {T}).\")\n",
    "            cube[p_idx, z_idx, :] = arr[:T]\n",
    "\n",
    "    # Reorder axes to (T, Z, P)\n",
    "    attributes = np.transpose(cube, (2, 1, 0)) # (T, Z, P)\n",
    "\n",
    "    if to_list:\n",
    "        return attributes.tolist(), zone_order\n",
    "    return attributes, zone_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_real_demand, _ = create_attributes_array_savedpred('actual', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LSTM, _ = create_attributes_array_savedpred('LSTM', 307, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "first_row = attributes_LSTM[0:1].copy()          # shape (1, 20, 1)\n",
    "attributes_LSTM = np.concatenate([first_row, attributes_LSTM], axis=0)\n",
    "\n",
    "attributes_SARIMA, _ = create_attributes_array_savedpred('SARIMA', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_SARIMAX, _ = create_attributes_array_savedpred('SARIMAX', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_TBATS, zone_order = create_attributes_array_savedpred('TBATS', 308,\n",
    "                            base_dir=\"Predictions/1_week_predictions\",\n",
    "                            zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_RF, _ = create_attributes_array_savedpred('RF', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDRF, _ = create_attributes_array_savedpred('LDRF', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_XGB, _ = create_attributes_array_savedpred('XGB', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDXGB, _ = create_attributes_array_savedpred('LDXGB', 308, quantiles=(25, 75),\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_QRF_median, _ = create_attributes_array_savedpred('QRF', 308, quantiles=(25,75),\n",
    "                                                             base_dir=\"Predictions/1_week_predictions\",\n",
    "                                                             zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDQRF_median, _ = create_attributes_array_savedpred('LDQRF', 308, quantiles=(25, 75), \n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_LDQRF, _ = create_attributes_array_savedpred('LDQRF', 308, quantiles=(25, 75),\n",
    "                                                     quantile_switch = True,\n",
    "                                                    base_dir=\"Predictions/1_week_predictions\",\n",
    "                                                    zone_order=zone_order, dtype=np.float32, to_list=False)\n",
    "\n",
    "attributes_QRF, _ = create_attributes_array_savedpred('QRF', 308, quantiles=(25, 75),quantile_switch = True,\n",
    "                        base_dir=\"Predictions/1_week_predictions\",\n",
    "                        zone_order=zone_order, dtype=np.float32, to_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_real_demand,\n",
    "                                                                                                zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                zone_wise_demand_pred = attributes_real_demand, \n",
    "                                                                                                zone_dict = zone_dict,\n",
    "                                                                                                method='constrained_k-means',\n",
    "                                                                                                prediction_type='actual')\n",
    "median_df['actual'] = list_of_cluster_demand_pred\n",
    "cluster_outcomes['actual_labels'] = list_of_labels\n",
    "cluster_outcomes['actual_cluster_demand'] = list_of_cluster_demand_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_real_demand,\n",
    "                                                                                                zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                zone_wise_demand_pred = attributes_real_demand, \n",
    "                                                                                                zone_dict = zone_dict,\n",
    "                                                                                                method='constrained_k-means',\n",
    "                                                                                                prediction_type='actual')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, list_of_cluster_demand_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, SARIMA_list_of_cluster_demand_actual, SARIMA_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_SARIMA,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_SARIMA, \n",
    "                                                                                                            zone_dict = zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='SARIMA')\n",
    "\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMA_list_of_cluster_demand_pred, print_out=True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMA_list_of_cluster_demand_actual, print_out=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, SARIMAX_list_of_cluster_demand_actual, SARIMAX_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_SARIMAX,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_SARIMAX, \n",
    "                                                                                                            zone_dict = zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='SARIMA')\n",
    "\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMAX_list_of_cluster_demand_pred, print_out=True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, SARIMAX_list_of_cluster_demand_actual, print_out=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, TBATS_list_of_cluster_demand_actual, TBATS_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_TBATS,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_TBATS, \n",
    "                                                                                                            zone_dict = zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='tbats')\n",
    "\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, TBATS_list_of_cluster_demand_pred, print_out=True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, TBATS_list_of_cluster_demand_actual, print_out=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, LSTM_list_of_cluster_demand_actual, LSTM_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_LSTM,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LSTM, \n",
    "                                                                                                            zone_dict = zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='LSTM')\n",
    "\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LSTM_list_of_cluster_demand_pred, print_out=True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LSTM_list_of_cluster_demand_actual, print_out=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, RF_list_of_cluster_demand_actual, RF_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_RF,\n",
    "                                                                                                    zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                    zone_wise_demand_pred = attributes_RF, \n",
    "                                                                                                    zone_dict = zone_dict,\n",
    "                                                                                                    method='constrained_k-means',\n",
    "                                                                                                    prediction_type='RF')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, RF_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, RF_list_of_cluster_demand_actual, print_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, LDRF_list_of_cluster_demand_actual, LDRF_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_LDRF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDRF, \n",
    "                                                                                                            zone_dict = zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='LDRF')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDRF_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDRF_list_of_cluster_demand_actual, print_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, XGB_list_of_cluster_demand_actual, XGB_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_XGB,\n",
    "                                                                                                        zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                        zone_wise_demand_pred = attributes_XGB, \n",
    "                                                                                                        zone_dict= zone_dict,\n",
    "                                                                                                        method='constrained_k-means',\n",
    "                                                                                                        prediction_type='XGB')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, XGB_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, XGB_list_of_cluster_demand_actual, print_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels, LDXGB_list_of_cluster_demand_actual, LDXGB_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_LDXGB,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDXGB, \n",
    "                                                                                                            zone_dict= zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='LDXGB')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDXGB_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDXGB_list_of_cluster_demand_actual, print_out = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Median/Single QRF 21 week'\n",
    "list_of_labels, medi_QRF_list_of_cluster_demand_actual, medi_QRF_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_QRF_median,\n",
    "                                                                                                                zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                                zone_wise_demand_pred = attributes_QRF_median, \n",
    "                                                                                                                zone_dict= zone_dict,\n",
    "                                                                                                                method='constrained_k-means',\n",
    "                                                                                                                prediction_type='QRF_median')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_QRF_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_QRF_list_of_cluster_demand_actual, print_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Quantile_QRF 21 week'\n",
    "list_of_labels, QRF_list_of_cluster_demand_actual, QRF_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_QRF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_QRF_median, \n",
    "                                                                                                            zone_dict= zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='QRF')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, QRF_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, QRF_list_of_cluster_demand_actual, print_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Median/Single LDQRF 21 week'\n",
    "list_of_labels, medi_LDQRF_list_of_cluster_demand_actual, medi_LDQRF_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_LDQRF_median,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDQRF_median, \n",
    "                                                                                                            zone_dict= zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='LDQRF_median')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_LDQRF_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, medi_LDQRF_list_of_cluster_demand_actual, print_out = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Quantile LDQRF 21 week'\n",
    "list_of_labels, LDQRF_list_of_cluster_demand_actual, LDQRF_list_of_cluster_demand_pred = CKMC_per_timestep(attributes_biglist = attributes_LDQRF,\n",
    "                                                                                                            zone_wise_demand_actual = attributes_real_demand,\n",
    "                                                                                                            zone_wise_demand_pred = attributes_LDQRF_median, \n",
    "                                                                                                            zone_dict= zone_dict,\n",
    "                                                                                                            method='constrained_k-means',\n",
    "                                                                                                            prediction_type='LDQRF')\n",
    "\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and predicted zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDQRF_list_of_cluster_demand_pred, print_out = True)\n",
    "print('\\n')\n",
    "print('Computing actual zone-wise resource from \"actual clusters\" and actual zone-wise resource from \"predicted clusters\"')\n",
    "clustering_evaluation(df_15min_, list_of_labels, list_of_cluster_demand_actual, LDQRF_list_of_cluster_demand_actual, print_out = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part_a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
